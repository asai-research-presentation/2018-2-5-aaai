#+title: Running a GOFAI in Deep Latent Space: Bridging the Sybsymbolic-Symbolic Boundary
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center

#+begin_larger
_Masataro Asai_, Alex Fukunaga, The University of Tokyo
#+end_larger

20+ min
#+end_center

#+begin_note
#+begin_alignright
Made by guicho2.71828 (Masataro Asai)
#+end_alignright
#+end_note
#+end_outline-text-1

* . 

#+begin_xlarge
#+begin_center
Hello, Machine Learning people!
#+end_center
#+end_xlarge

#+begin_center
Deep Learning, RL, data mining, etc...
#+end_center

#+begin_resume
First of all, hello machine learning people.
Im not a machine learning person.
Im usually studying a type of symbolic AI and I visited here expecting an interesting conversation with
machine learning professionals.
#+end_resume

* . 

#+begin_xlarge
#+begin_center
Hello, Symbolic AI people!
#+end_center
#+end_xlarge

#+begin_center
SAT, CP, Logic, TCS, ...
#+end_center

#+begin_resume
And also, hello other symbolic AI people.
Although I am not doing exactly the same thing, we have common roots.

Let me give you one question before presenting my paper...
#+end_resume

* . 

#+begin_xlarge
#+begin_center
Today, I talk about how to integrate DL and logic-based systems.
#+end_center
#+end_xlarge

* . 

#+begin_xlarge
#+begin_center
I am from */AI Planning/* community (symbolic)
#+end_center
#+end_xlarge

#+begin_center
[[sgif:icapslogo]]

International Conference on Automated Planning and Scheduling

(AAAI sister conference, 33% avg. accept ratio, since 1990, ECP+AIPS→ICAPS)
#+end_center

* Planning people are working on */Automated Planners/*            :needsPic:

+ Compute the *high-level plan* for an agent: /plan = (action_1, action_2, ... action_n)/
  + Search-based planning projects many steps further into the future than RL
  + ↔ reflex (0-step), limited deliberation (1,2-steps)
+ High-dimensional, *implicit* state space search
  + Entire space does not fit in memory, only *transition rules* are given
+ STRIPS-like *PDDL modelling language*, based on
  + 1st-order logic, close-world assumption
+ *Soundness, completeness, optimality* matters
  + Expensive Applications: Satellite operations (Deep Space 1), Mars Exploration Rover (MER), underwater vehicles

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*/Backgrounds/*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* High-level Planning: Sliding tile puzzle (a.k.a 8-puzzle)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
+ Slide a tile to an empty place (each step)
+ → make the tile placement identical to the goal
+ → (abstracts the low level control e.g. arm motion)
+ *Search states* represented by conjunctions of 1st-order logic (objects, predicates, propositions)
  #+begin_src lisp
  (empty x0 y0)     ; or, empty(x0, y0)
  (is panel6 x1 y0) ; or, is(panel6, x1, y0)
  (up    y0 y1)... (down  y1 y0)...
  (right x0 x1)... (left  x0 x1)...
  #+end_src
#+end_span8
#+begin_span2
initial state
[[png:8puzzle-standard]]
#+end_span2
#+begin_span2
goal state
[[png:8puzzle-standard-goal]]
#+end_span2
#+end_row-fluid
#+begin_row-fluid
#+begin_span12
+ To symbolic systems *names do not matter* (just matters to human) as long as there is consistency
  #+begin_src lisp
  (car   banana0 kiwi0)           ; numbers (0,1) don't matter either
  (shark bogo6 banana1 kiwi0)     ; just to make sense for us
  (apple  kiwi0 kiwi1)... (pinapple kiwi1 kiwi0)...
  (pen    banana0 banana1)... (applepen banana0 banana1)...
  #+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

** High-level Planning Task: State Transitions

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
How to represent "sliding up tile 7" ? → *actions*

#+begin_quote
Transition Rules of "sliding up":

+ You can slide up a tile only to an empty location.

+ When you slide it, the orignal location becomes empty.

+ New location is no longer empty, is occupied by the tile.
#+end_quote
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
# #+begin_span2
# goal state
# [[png:8puzzle-standard-goal]]
# #+end_span2
#+end_row-fluid
#+begin_row-fluid
#+begin_span12
+ When /Empty(x, y_{old}) ∧ is(panel, x, y_{new}) ∧ up(y_{new}, y_{old})/ ;
  
  then /¬ Empty(x,y_{old}) ∧ Empty(x,y_{new}) ∧ ¬ is(panel, x, y_{new}) ∧ is(panel, x, y_{old})/

+ 
  #+begin_src lisp
  ;; Translates to a PDDL model below:
  (:action slide-up ...
   :precondition (and (empty ?x ?y-old)
                      (is ?panel ?x ?y-new) ...)
   :effects (and (not (empty ?x ?y-old))     (empty ?x ?y-new)
                 (not (is ?panel ?x ?y-new)) (is ?panel ?x ?y-old)))
  #+end_src

# #+begin_src lisp
# (:action move-up
#  :parameters (?x ?y-old ?y-new ?panel)
#  :precondition (and (empty ?x ?y-old)
#                     (up ?y-old ?y-new)
#                     (is ?panel ?x ?y-new))
#  :effects (and (not (empty ?x ?y-old))
#                (empty ?x ?y-new)
#                (not (is ?panel ?x ?y-new))
#                (is ?panel ?x ?y-old)))
# #+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

+ 
  #+begin_larger
  #+begin_alignright
  But *where does this representation come from?*
  #+end_alignright
  #+end_larger

** PDDL representations come from */human encoding/*.

#+begin_quote
*Knowledge-Acquisition Bottleneck (Cullen, 1988)*:

The *cost of human involved* for converting real-world problems into inputs for
domain-independent *symbolic* systems
#+end_quote

To automate this process, you need *two* elements:

+ *Symbols Grounding: Finding identifiable entities called /symbols/*

  | Types of symbols  |                                            |
  |-------------------+--------------------------------------------|
  | Object symbols    | panel7, x_0, y_0 ...                       |
  | Predicate symbols | (*empty* ?x ?y) (*up* ?y_0 ?y_1)           |
  | Propositions      | *p_28* = (empty x_0 y_0) --- application |
  | Action symbols    | (*slide-up* panel_7 x_0 y_1)               |

+ *Action Model Acquisition (AMA): Understands how the world evolve*
  
  #+begin_center
  When /Empty(x, y_{old}) ∧ .../ ; Then /¬ Empty(x,y_{old}) ∧ / ...
  #+end_center

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

** Can we solve an 8-puzzle optimally, w/o human encoding?

#+begin_center
+ 3x3 Image-based Sliding Tile Puzzle: maximum 31 steps *optimal* plan

+ 4x4、5x5 : infeasible under blind search (memory exhaust)

+ Symbolic systems can compute *optimal* solutions w/ A* + admissible heuristics
#+end_center

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+ 
   #+begin_center
   *BUT*

   *WE*

   *DO*

   *NOT*

   *HAVE*

   */SYMBOLS/*

   *NOR*

   */AN ACTION MODEL/!*
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

* Backgrounds

#+begin_xlarge
Survey of Exisiting Action Model Acquisition Techniques
#+end_xlarge

　

#+begin_alignright
i.e. Systems that find action models
#+end_alignright

** Limitations of Existing Systems                                 :noexport:

#+begin_xlarge
#+begin_center
So far, ALL existing AMA systems require */symbolic / near-symbolic, accurate state inputs/* and/or */discrete action labels/*.
#+end_center
#+end_xlarge

#+begin_alignright
i.e. They need symbols to find an action model
#+end_alignright

** So far, ALL existing AMA systems require */symbolic inputs/*

ARMS (Yang AIJ07)
LOCM (ICAPS09)
Argall (AIJ09)
Mourao (UAI12)

All taking the *symbolic* inputs to find the *action models*

[[spng:locm]]

** Framer (ICAPS17)                                                :noexport:

*Near-Symbols* : Parses natural language sentences with a *clear grammatical structure*.


#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[spng:framer]]
#+end_span6
#+begin_span6
+ Alleviates the burden of domain experts, but *still requires human*
+ Not handling "Natural Language":
  
  #+begin_quote
  Pick up that parcel over there ... yeah, it has a label on it, it says Parcel1, you can see
  it from here, the Location B. Then put it in the car, I mean the truck, the red one.
  #+end_quote
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Konidaris, Kaelbring (AAAI14, IJCAI15)                          :noexport:

"Constructing Symbolic Representations for High-Level Planning" (AAAI14)

+ What it does :: Converting a *Semi-MDP Model* to a *PDDL Model* by set-theoretic representation
                  
                  i.e. *Model-to-Model* conversion, not *generating a model from the scratch*
+ Semi-MDP contains Action Labels :: =move= and =interact= (Playroom)
+ Sensor inputs are structured (Labels for "State Variable" are known) :: 

     x/y-distance, light level, whether a monkey cries
     
     → Each sensor has a distinct meaning (no overwrap)

# + Low-dimensional, accurate input :: 33 vars (Playroom), 9 vars (Treasure), no noise
#      
#      Although IJCAI15 shows "visual depiction", it is not used by the system

** Learning from Video for Board Game (Bardu ICRA10; Kaiser AAAI12; Kirk 16) :noexport:

*Handles Images, but with strong assumptions (almost symbol)* e.g.

#+begin_quote
Tic-Tac-Toe with *Ellipse Detectors* (Bardu 10)
     
→ Almost immediately provides propositions

→ Also, Domain-dependent ("3x3 grid" "Ellipse" are hard-coded)
#+end_quote

* Problem Description

#+begin_center
We do not have */SYMBOLS/*

*/So we cannot apply existing AMA methods!/*
#+end_center

[[sjpg:puzzle]]

** Task: Solving an */Imaged-Based/* 8-puzzle w/o Prior Explicit Knowledge

*No Prior Knowledge* : labels/symbols such as "9 tiles", "moving"

[[sjpg:puzzle]]

** Task: Solving an */Imaged-Based/* 8-puzzle w/o Prior Explicit Knowledge

*/No Prior Knowledge/* : labels/symbols such as "9 tiles", "moving"

[[sjpg:puzzle]]

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

#+begin_larger
*/No Prior Knowledge/* : */Domain-independent Image-based planner/*
#+end_larger

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
Tower of Hanoi

[[sjpg:hanoi]]
#+end_span6
#+begin_span4
Lights-Out

[[sjpg:lightsout]]
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Input1: Training Inputs -- Image Pairs

Image pairs showing the before/after states of valid actions (randomly sampled)

[[png:overview/1]]

** Input1: Training Inputs -- Image Pairs

[[png:overview/2]]

** Input2: Planning Inputs -- Initial Image & Goal Image

Visual depiction of the initial state and a single goal state

[[png:overview/input2]]

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

#+HTML: <embed src="img/overview/3.svg" type="image/svg+xml"  />

#+begin_larger
　

　
#+end_larger

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

#+HTML: <embed src="img/overview/3-hanoi.svg" type="image/svg+xml"  />

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*/Latplan Architecture/*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Latent-Space Planner (*/LatPlan/*) architechture

 [[png:overview/planning1]]

** Step 1: Propositional Symbol Grounding

 [[png:overview/planning2]]

*** Step 1: State Autoencoder                                      :noexport:

[[png:train-state-ae]]

Trained SAE provides two functions:

+ $b = Encode(r)$ *maps a raw datum $r\;$ to a bit vector $b\;$ (propositions)*

+ $\tilde{r} = Decode(b)$ *maps a bit vector $b\;$ to a raw datum $\tilde{r}$*

** Step 2: Action Model Acquisition (AMA)

 [[png:overview/planning3]]

** Step 3: Solve the Symbolic Planning Problem

  [[png:overview/planning4]]

** Step 4: Executing the Symbolic Plan

  [[png:overview/planning5]]

** Step 5: Obtaining the Visualization

  [[png:overview/planning6]]

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*/State AutoEncoder (SAE)/*
#+end_xlarge

break

#+begin_xlarge
*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* State AutoEncoder (SAE)

SAE is a *neural network* which provides two functions:

+ $b = Encode(r)$ : *maps a raw datum $r\;$ to a bit vector $b\;$ (propositions)*

+ $\tilde{r} = Decode(b)$ : *maps a bit vector $b\;$ to a raw datum $\tilde{r}$*

#+begin_larger
+ *A bidirectional mapping between*

  + *subsymbolic representation* (image array) and

  + *symbolic representation* (bit vectors, propositional variables)
#+end_larger

** Neural Network 101                                              :noexport:

[[png:deeplearning/1]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/2]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/3]]

** Stochastic Gradient Descent + GPU                               :noexport:

[[spng:gradient-descent]]

Plus misc techniques e.g. *Batchnorm*, *Dropout*

#+begin_larger
*Pretty much everything is on the standard online tutorial / lecture cource / MOOP*

*Good libraries --- Tensorflow, Keras* --- you can learn in 1-2 months
#+end_larger

** NN for Standard Classification Tasks (Supervised)

Target Function $y=f(x)\;$ trained by SGD minimizing $|y-f(x)|$

#+begin_alignright
SGD: Stochastic Gradient Descent
#+end_alignright
  
| Task                 | Input x  | Output y                           |
|----------------------+----------+------------------------------------|
| Image classification | Image    | Label (1=car, 2=cat, 3=monkey ...) |
# | Translation          | Sentence | Sentence                           |
# | Go eval. function    | State    | Number                             |

#+begin_larger
 + *This is not suitable for our task;*
   + */there are no labels/* provided by humans, i.e.
   + */there are no real answers/* for how to ground propositional symbols
     
     #+begin_alignright
     (*even people do it differently on a different language*.

     cf. Snow in Eskimo language)
     #+end_alignright

#+end_larger

** Unsupervised Learning by AutoEncoder

# Auto = "self" --- Autoencoding = "encoding itself"

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
　

Target Function: Identity $x=f(x)$
+ Encode $x\;$ to a *latent vector* $z$
+ Decode $z\;$ back to the input $x$
+ $z\;$ has a smaller dimension
+ i.e. Compression: $X \leftrightarrow Z$
+ Training: Minimize $|x - f(x)|\;$
  (*reconstruction loss*)
#+end_span6
#+begin_span6
[[png:deeplearning/autoenc]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
#+begin_larger
+ → However, */✘ Latent vector Z is real-valued/*

  */INCOMPATIBLE, UNUSABLE for propositional reasoning/*
#+end_larger
#+end_alignright

** Variational AutoEncoder (VAE)                                   :noexport:

An AutoEncoder that *enforce a certain distribution* on $Z \subset \mathbb{R}^n$ over the dataset $X$

#+begin_quote
You have $X=$ { 10k images of apples }. If you train a *Gaussian VAE* on $X$, then $Z = Encode(X) \approx N(\mu,\sigma)$ for some $\mu,\sigma \in \mathbb{R}^n$.
#+end_quote

VAE needs a *reparametrization trick* because random distributions are non-differentiable.

#+begin_quote
Reparametrization for $N(\mu,\sigma)$: $\mu + \sigma N(0,1)$

#+begin_center
\mu and \sigma are differentiable vectors, $N(0,1)$ is not.
#+end_center
#+end_quote

** Gumbel-Softmax VAE (Jang, Gu, ICLR2017)

*Additional optimization penalty* which enforces $Z \sim \textbf{Categorical}$:

　　　→ $z\;$ converges to a 1-hot vector e.g.  $\langle 0,0,1,0 \rangle$ .

Example: Represent an MNIST image with 30 variables of 8 categories.

#+begin_center
 #+begin_html
 <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="649px" height="206px" version="1.1" content="&lt;mxfile userAgent=&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot; version=&quot;6.0.1.2&quot; editor=&quot;www.draw.io&quot; type=&quot;google&quot;&gt;&lt;diagram name=&quot;Page-1&quot;&gt;3ZhLc5swEMc/Dcd0kAQCX+O67SGd6TTTaXqU0fJoZcsjy69++oogDBjs0MZ2TONDpL9ey29Xi5BDxrPtR8UW6WfJQTjY5VuHvHcwxshF5l+u7AqF+rgQEpXxQkKV8Jj9Biu6Vl1lHJaNjlpKobNFU4zkfA6RbmhMKblpdoulaK66YAm0hMeIibb6PeM6LdTQdyv9E2RJWq6MXNsyY2VnKyxTxuWmJpGJQ8ZKSl2UZtsxiBxeyaUY9+FI694wBXPdZ0AQh1EYx4xP46kXRHBnZ1gzsbIPaw3Vu/LpN2mm4XHBory+MR52yH2qZ8LUkCnGmRBjKaR67k0AcR8Coy+1kr+g1jKiAWHUtCi5mnPgdrw1AJSG7dGnQntWJshAzkCrneliB5CAFENsfCHPL+qbylt7n6Q1T+HAisxGSLKfu4JoCpZjT6a4xRT71MFUmFXvebY2xSQvfoWHb6VsFqm1tHygUjmbroyR9y944ww08QFN7LktmmEHzPASLEmLpY/wcFiiW2LptVAAN3nOVqXSqUzknIlJpdb2qtuEA9tMP9XKP/Iu7/y8NjeGPtkRz5Wq7SdovbMJnq20NFK17oOUiwb63LzT4M3TyJWK4HT0aKYS0Kd2a9uBCgTT2bq5/mvc4eEIU4xG4PsesCDqCO1z+ufKpBHugZq8FeorRf6VmfdBfiQ9XR45HXbi9kf4zRJ3i2Uw7AOFd0ssw//9JXgseupponu3Ht8MwYEDL+KZ0cDehyeBBqd3BB31B2pn+SIzY0D/KYqYsKMO3LK36N88hc7+1cgZhHHU9dVIoxCm8SW+GukINQm67bSEaEde2otnDX901nAfRGaiPTYSutYJ5uiRdthRfldeot1IlPfI8i9Abcb8AeLYz3+diJ//Xgm1hIg6IHpdEL1LQCxvEwcK0Q4I3IP8+8ZQX/9SuwGoJGxud+STFtSQtpli3/trpqZa3VwX54rq/p9M/gA=&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g transform="translate(0.5,0.5)"><rect x="288" y="0.75" width="75" height="202.5" rx="11.25" ry="11.25" fill="#e1d5e7" stroke="#9673a6" pointer-events="none"/><path d="M 243 72 L 273 102 L 243 132 L 213 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(231.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 168 72 L 198 102 L 168 132 L 138 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(156.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 198 102 L 208.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 212.16 102 L 206.91 104.63 L 208.22 102 L 206.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 120.75 102 L 135.75 102 L 123 102 L 133.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 137.16 102 L 131.91 104.63 L 133.22 102 L 131.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 273 102 L 288 102 L 273 102 L 283.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 287.16 102 L 281.91 104.63 L 283.22 102 L 281.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 482.25 72 L 512.25 102 L 482.25 132 L 452.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(470.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 407.25 72 L 437.25 102 L 407.25 132 L 377.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(395.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 437.25 102 L 447.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 451.41 102 L 446.16 104.63 L 447.47 102 L 446.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 360 102 L 375 102 L 362.25 102 L 372.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 376.41 102 L 371.16 104.63 L 372.47 102 L 371.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="526.5" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><path d="M 512.25 102 L 521.72 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 525.66 102 L 520.41 104.63 L 521.72 102 L 520.41 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="0.75" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/>
 <image xlink:href="img/static/x0.gif" x="8.25" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/x1.gif" x="534" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/y.gif" x="293.25" y="6.75" width="64.5" height="190.5" fill="#f5f5f5" stroke="#666666" pointer-events="none"/></g></svg>
 #+end_html
#+end_center

#+begin_larger
#+begin_center
+ Key idea: *These categorical variables are /directly/ usable*

  *as the source of propositional models*

  In particular, *2 categories → propositional variables (true/false)*
#+end_center
#+end_larger

** State Autoencoder (*/before training/*)

 [[png:sae/state-ae-before]]

** State Autoencoder (_/after training/_)

 [[png:sae/state-ae]]

** Gumbel-Softmax: Differential Approximation of Gumbel-Max        :noexport:

#+begin_larger
It uses *annealing* to approximate discrete vectors
#+end_larger

#+begin_smaller
Gumbel-Max: Method for drawing one-hot vector sample ($z$) from category probability ($x$)

+ E.g.: $x=[0.1, 0.1, 0.8] \rightarrow z = [1,0,0] \text{or} [0,1,0] \text{or} [0,0,1]$

+ $z = \text{ GumbelMax}(x) = [ i == \arg \max_j (\text{ Gumbel}(0,1)+\log x_j) \; ? \; 1 : 0 ]$

+ argmax is non-differentiable → softmax approximation (differentiable)

+ $z = \text{ GumbelSoftmax}_\tau (x) = \text{ Softmax}( [\text{ Gumbel}(0,1)+\log x_j]/\tau )$

+ Temparature $\tau \rightarrow 0$ , $z\rightarrow \text{one-hot vector}$

  # \[
  # \text{ GumbelSoftmax}_\tau (x) \rightarrow \text{ GumbelMax}(x) \quad (\tau\rightarrow 0)
  # \]
#+end_smaller

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span8
#+begin_center
[[png:sae/gumbel]]
#+end_center
#+end_span8
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

#+begin_note
Maddison et. al., 2014
#+end_note

* Verifying the feasibility of Latplan system and SAE

#+begin_quote
Q: Do the *propositions* found by the SAE (neural network) *make any logical sense?*

Q: Is it possible to *reason over such generated propositions?*
#+end_quote

We need to build a *minimum-working Latplan system*

#+begin_center
Planner needs an *action model* (transition rules), not just propositions
#+end_center

#+begin_alignright
#+begin_larger
So we built *AMA ver.1*, a trivial, placeholder AMA system
#+end_larger
#+end_alignright

** AMA_1 : a */trivial, placeholder/* AMA method

It does not actually *learn/generalize* anything

　

 [[png:overview/ama1]]

** Embed Directly: One Action Per Transition

#+begin_src lisp
 0011 → 0101  

　　　↓       ;; one action per transition
(:action       action-0011-0101
               ;; before-state is embedded directly
 :precondition (and (b0-false) (b1-false) (b2-true) (b3-true))
               ;; effect = state diff
 :effect       (and (not (b1-false)) (b1-true)
                    (not (b2-true))  (b2-false)))
#+end_src

Conversion from a bit to a proposition:

#+begin_center
 $i$-th bit is 1 → Proposition ($b_i$ -true)

 $i$-th bit is 0 → Proposition ($b_i$ -false)
#+end_center

*** Example PDDL

# Examples in $N=25$ (in the paper we bypassed PDDL-SAS translater)

#+begin_src lisp
(define (domain latent)
 (:requirements :strips)
 (:predicates (b0-true) (b0-false) (b1-true) ... (b24-false))

 (:action a10000010010110111100011111000010001011111110011111
  :parameters () :precondition
  (and (b0-true) (b1-false) (b2-false) ... (b24-true))
  :effect (and (not (b5-false))  (b5-true)
               (not (b6-true))   (b6-false)
               (not (b13-false)) (b13-true)
               (not (b20-false)) (b20-true)))

 (:action a10000010010110111100011110000001001011011110001110
  ...
#+end_src

** Step 3: Solve the Symbolic Planning Problem

  [[png:overview/planning4]]

** Step 4: Executing the Symbolic Plan

  [[png:overview/planning5]]

** Step 5: Obtaining the Visualization

  [[png:overview/planning6]]

* AMA_1 Experiments

+ SAE: *trained with 20k images* (note: > 360k entire states in 8-puzzle)

  + */SAE is generalizing/*

+ AMA_1: requires the entire transistions (note: > 1M transitions in 8-puzzle)

  + _/AMA_1 is NOT generalizing/_

+ Planner: a State-of-the-Art, Fast Downward (Helmert, 08), $A^*$ *(optimal search)*

  + *It must find an optimal solution*

+ Runtime: ~3sec (instances are too small for symbolic systems)

** 8-puzzle Results with MNIST tiles (MNIST 8-puzzle)

 8-puzzle using digits from MNIST database

 [[png:results/mnist-plan]]

 #+begin_larger
 An instance whose optimal solution length is known
 #+begin_xlarge
 #+begin_alignright
  → *31 step optimal plan*
 #+end_alignright
 #+end_xlarge
 #+end_larger

** Results with photographic, unseparated tiles (Mandrill 8-puzzle)

MNIST 8-puzzle has cleanly separated objects -> This domain does not.

[[png:results/mandrill-intro]]

** Results with photographic, unseparated tiles (Mandrill 8-puzzle)

[[png:results/mandrill-plan]]

#+begin_xlarge
#+begin_alignright
 → *Optimal Solution*
#+end_alignright
#+end_xlarge

** Results with photographic, unseparated tiles (Spider 8-puzzle)

[[png:results/spider-plan-new]]

#+begin_center
#+begin_larger
Notice that *latplan has no idea that this is an 8-puzzle*; Thus MNIST, Mandrill, Spider are entirely different domains (state encoding is also different)
#+end_larger
#+end_center

** Tower of Hanoi (3 disks, 4 disks)

Completely different puzzle problems can be solved by the same system

[[png:results/hanoi3]]

[[png:results/hanoi4]]

#+begin_alignright
#+begin_xlarge
 → *Optimal Solution* (7 steps,15 steps)
#+end_xlarge
#+end_alignright

** Lights Out

Completely different puzzle problems can be solved by the same system

[[png:results/lightsout_new4x4]]

#+begin_alignright
#+begin_xlarge
 → *Optimal Solution*
#+end_xlarge
#+end_alignright

** Twisted Lights Out

Does not assume grid-like structures

[[png:results/lightsout_twisted_new4x4]]

#+begin_alignright
#+begin_xlarge
 → *Optimal Solution*
#+end_xlarge
#+end_alignright

** Handling the Noisy Input

SAE implementation uses a denoising layer (Denoising AE)
# Denoising AE を使っているため入力ノイズに左右されずにプランを求められる

[[png:results/noise-new]]

#+begin_larger
#+begin_alignright
 → *Benefit from existing DL methods immediately*

 → *Improved speed, robustness, accuracy*
#+end_alignright
#+end_larger
* Why bother the off-the-shelf planner? Shouldn't the blind search do? :noexport:

*Domain-independent lowerbounds works, /SURPRISINGLY!/*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ This is *NOT* a trivial finding!

  lower-bounds are...

  + taylored for *man-made* domains
  
  + assumes the domain has a *structure*

  Blind search even sometimes outperform sophisticated methods on man-made instances (Edelkamp 12)

+ lb works → *the more difficult problems can be solved (future work)*
#+end_span6
#+begin_span6
#+begin_smaller

| domain          | Dijkstra   | A*+PDB          | instances |
|-----------------+------------+-----------------+-----------|
| MNIST           | 210k(50k)  | *97k* (53k)     |        13 |
| Mandrill        | 176k(13k)  | *112k* (57k)    |        12 |
| Spider          | 275k(65k)  | *58k* (30k)     |        13 |
| Hanoi (4 disk)  | 20.7(29.7) | *12.6* (22.0)   |        30 |
| LightsOut (4x4) | 433(610.4) | *130.3* (164.4) |        16 |
| Twisted   (4x4) | 398(683.1) | *29.3* (62.4)   |        16 |
|-----------------+------------+-----------------+-----------|

#+end_smaller
#+end_span6
#+end_row-fluid
#+end_container-fluid

* State Autoencoder Conclusion

+ SAE can learn from small examples ::

     20k training images → learn to map 360k unseen images

     (AMA_1 is trivial, though)

+ SAE-based propositions are sound ::

     Planners can reason over the generated propositions

+ Latplan maintains the theoretical guarantee in the search algorithm ::

     Given the complete state space graph,

  + Optimising algorithm (A*) returns an optimal solution

  + Completeness is guaranteed

* *** ☕ Break ☕ ***

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

*/break/*

#+begin_xlarge
*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*/State AutoEncoder (SAE)/*
#+end_xlarge

break

#+begin_xlarge
*/Action AutoEncoder (AAE)/*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Latplan Feasible! Now what?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
*Providing the entire transitions is impractical*, cannot afford such data in the real world (e.g. autonomous vehicle)

*Learning / generalizing AMA with small examples is necessary*

#+end_span6
#+begin_span6
 [[png:overview/ama1]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_larger
#+begin_alignright
→ AMA_2, a *novel neural architecture* which learns

 *Action Models (action symbols, preconditions, effects)*
#+end_alignright
#+end_larger

* Typical domains contain a set of */Action Symbols/*
* Action symbols

+ Represent a *type* of transition (→ generalization)

+ E.g. *slide-up-tile7-at-x0-y1* action abstracts all transitions that looks like this

+ → not merely an individual transition

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition ...
 :effects      ...)
#+end_src
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

** Latplan Training Input lacks Action Labels

+ Note: Existing AMA methods all require action symbols

+ *Latplan training input (transitions) consists of just pairs of images*

+ Without action symbols, it has *no idea which transitions are of the same type*

  #+begin_center
  ↓
  #+end_center

  [[png:ama/action-symbol]]

** Is that a problem?

+ Without action symbols ::

     SAE with $|z|=36 \text{bit}$

  + → you should enumerate the *entire $2^{36}$ states as successors*,
  + → then remove the invalid transitions with AD.

+ With a limited number of action symbols ::

     We can */enumerate the successors in constant time/*.

#+begin_alignright
#+begin_larger
+ *AAE finds these action labels without human supervision*
#+end_larger
#+end_alignright

** Is AAE trivial?

#+begin_xlarge
Case 1: Linear Search Spaces
#+end_xlarge

+ *We do not need an action label*; it is linear anyways
+ AMA reduces to a *prediction task*
  + We can train a neural network $a(s) = t\;$ by minimizing $|t-a(s)|$

[[png:ama/linear]]

** Is AAE trivial?

#+begin_xlarge
Case 2: Graphs
#+end_xlarge

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
+ *Multiple action labels*
+ *Varying number of successors*

  *for each state*

　

+ *Can you train $a(s) = t\;$ for each action ?*
+ 
  #+begin_center
  *NO*, you don't know *which transition belongs to which action.*

  */Everything is mixed up./*
  #+end_center

#+end_span7
#+begin_span5
[[png:ama/non-linear]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

* AMA_2 Overview                                                   :noexport:

2 neural networks, *Action Autoencoder* (AAE) and *Action Descriminator* (AD)

+ AAE finds *action symbols* and *action effects*
  + Able to *enumerate the successor candidates* of the current state
  + Some candidates represent the wrong transitions
+ AD finds *action preconditions*, i.e. which transitions are allowed (valid)
  + Able to *filter the wrong successors*
+ Combined, AAE and AD *jointly learns a successor function* from a small dataset
  + *Directly usable by a forward state space search*

* AMA_2 Overview

[[png:ama/overview0]]

** AMA_2 Overview

 [[png:ama/overview1]]

** AMA_2 Overview

 [[png:ama/overview2]]

** AMA_2 Overview

 [[png:ama/overview3]]
* Action AutoEncoder

[[png:aae/aae]]

** Typical AE vs AAE

[[png:aae/aae2]]

* Action Discriminator

A simple binary classifier that tells whether (s,t) is a valid transition

(actually returns a probability; "valid" when > 0.5)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:aae/ad]]
#+end_span6
#+begin_span6
+ *Binary classifier*: trained by a positive and a negative dataset
+ *But hey, wait!*

#+begin_center
+ *Where are the negative dataset?*

+ *What are the invalid moves?*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_center
+ *Invalid moves are /just moves that are not valid! We have no idea!/*
#+end_center

#+begin_alignright
+ Surprisingly, this is rather a *fundamental* question... 
#+end_alignright

** Negative examples are NEVER observed

Do robots have a chance to observe *teleportation* in the real world?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span8
[[png:aae/teleportation]]
#+end_span8
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid


+ *No*, since teleportation *violates the laws of physics*.

  #+begin_alignright
  #+begin_smaller
  (at least, in a macro scale, believed to be almost impossible)
  #+end_smaller
  #+end_alignright

+ For a robot operating in a real world, invalid transitions are *never* observed.

+ 
  #+begin_center
  The system should *imagine/generate* the false/invalid transitions
  (e.g. teleportation) in order to train the binary classifier (like in a thought
  experiment), *but we do not know what is valid NOR what is /invalid/.*

  #+begin_larger
  */We have no way to generate them!/*
  #+end_larger
  #+end_center

** PU-Learning framework (Elkan & Noto, KDD 08')

A method to learn a *positive/negative classifier* from the *positive/mixed datasets*

Mixed dataset: some are positive, some are negative, unknown ratio

We have:

+ Positive examples: Training input: *all observed data are guaranteed to be valid*
+ Mixed examples: *All successor candidates generated by the AAE*

#+begin_alignright
+ Now we can run the PU-learning and train AD!
#+end_alignright

** Successor Function

AAE enumerates the potential successors; AD/SD removes the invalid successors.

\begin{align*}
  Succ(s) &= \{t = apply(a,s) \; | \; a \in \{0\ldots 127\} \setminus \textit{unused},\\
          & \qquad \land AD(s,t) \geq 0.5 \\
          & \qquad \land SD(t) \geq 0.5 \}
\end{align*}

# & \qquad \land SD(t) \geq 0.5 \\
# & \qquad \land Encode(Decode(s)) \equiv s \\
# & \qquad \land Apply(Action(t,s),s) \equiv t \}

* AMA_2 Experiments 1

Is it feasibile to do planning with AAE and AD?

+ 100 instances for each domain
  + for each noise type (std, gaussian noise, salt/pepper noise)
+ self-avoiding random walks from the goal state
  + (benchmark A) 7-step
  + (benchmark B) 14-step
+ 180 sec. time limit
+ Domain-specific plan validators.

# The failures are due to timeouts
# (the successor function requires many calls to the feedforward neural nets,
#  resulting in a very slow node generation).

# We next examine the accuracy of the AD and SD (\reftbl{tab:aae-results}).
# We measured the type-1/2 errors for the valid and invalid transitions (for AD) and states (SD).
# Low errors show that our networks successfully learned the action models.

** Results

G: Gaussian noise, s/p: salt/pepper noise

Noise are applied to the planning inputs (init/goal images)

| /          |   < |     |   > |   < |    |   > |
| step       |   7 |   7 |   7 |  14 | 14 |  14 |
| noise      | std |   G | s/p | std |  G | s/p |
|------------+-----+-----+-----+-----+----+-----|
| MNIST      |  72 |  64 |  64 |   6 |  4 |   3 |
| Mandrill   | 100 | 100 | 100 |   9 | 14 |  14 |
| Spider     |  94 |  99 |  98 |  29 | 36 |  38 |
| LightsOut  | 100 |  99 | 100 |  59 | 60 |  51 |
| Twisted LO |  96 |  65 |  98 |  75 | 68 |  72 |
| Hanoi      |  37 |  44 |  39 |  15 | 18 |  17 |

* AMA_2 Experiments 2                                              :noexport:

How accurate are Action Discriminators and State Discriminators?


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
Measure the type-1 / type-2 error in %

#+begin_smaller
|          |    SD |    SD |   |    AD |    AD |   AD |   AD |
|          | type1 | type2 |   | type1 | type2 | 2/SD |  2/V |
|----------+-------+-------+---+-------+-------+------+------|
| MNIST    |  0.09 | <0.01 |   |  1.55 |  14.9 | 6.15 | 6.20 |
| Mandrill | <0.01 | <0.01 |   |  1.10 |  16.6 | 2.93 | 2.94 |
| Spider   | <0.01 | <0.01 |   |  1.22 |  17.7 | 4.97 | 4.91 |
| L. Out   | <0.01 |   N/A |   |  0.03 |  1.64 | 1.64 | 1.64 |
| Twisted  | <0.01 |   N/A |   |  0.02 |  1.82 | 1.82 | 1.82 |
| Hanoi    |  0.03 | <0.01 |   |  0.25 |  3.50 | 3.79 | 4.07 |
#+end_smaller

#+end_span7
#+begin_span5
#+begin_smaller
+ (SD type-1) :: Generate all valid states and count the states misclassified as invalid.
+ (SD type-2) :: Generate reconstructable states, remove the valid states (w/ validator),
                 sample 30k states, and count the states misclassified as valid.
                 N/A means all reconstructable states were valid.
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
+ (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
                 invalid according to SD or the validator.
#+end_smaller
#+end_span5
#+end_row-fluid
#+end_container-fluid

* AMA_2 Experiments 2

How accurate are Action Discriminators?


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
Measure the type-1 / type-2 error in %

|          | type1 | type2 | 2/SD |  2/V |
|----------+-------+-------+------+------|
| MNIST    |  1.55 |  14.9 | 6.15 | 6.20 |
| Mandrill |  1.10 |  16.6 | 2.93 | 2.94 |
| Spider   |  1.22 |  17.7 | 4.97 | 4.91 |
| L. Out   |  0.03 |  1.64 | 1.64 | 1.64 |
| Twisted  |  0.02 |  1.82 | 1.82 | 1.82 |
| Hanoi    |  0.25 |  3.50 | 3.79 | 4.07 |

#+end_span7
#+begin_span5
#+begin_smaller
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
+ (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
                 invalid according to SD or the validator.
#+end_smaller
#+end_span5
#+end_row-fluid
#+end_container-fluid

* Conclusion

+ *Latplan Architecture* 

  　　Automatically models / solves the real-world problem as a symbolic search
  # + Input : Unlabelled pairs of images, initial image, goal image
  # + Output : Visualized plans to achieve the goal
+ *State AutoEncoder(SAE)* : */real-world states/ ↔ /propositional states/*
+ *Action AutoEncoder(AAE)* : */transitions/ ↔ /action labels/*
+ *Action Discriminator(AD)* : */transition/ → /bool/* with *PU-Learning*

#+begin_center
+ *Two* of the major grounding problems were solved!
  | Types of symbols      |                              |
  |-----------------------+------------------------------|
  | Propositional symbols | *Solved!*                    |
  | Action symbols        | *Solved!*                    |
  | Object symbols        | R-CNN (Computer Vision) etc? |
  | Predicate symbols     | ???                          |
#+end_center

** Future Work (input format)

LatPlan is an *architecture* : Any system with SAE is a LatPlan implementation

*Different SAE allows LatPlan to reason about different types of raw data*

+ Autoencoders for *unstructured text* [Li et.al. 2015], *audio* [Deng, Li, et al. 2010]
+ Examples:
  + Transition rule *This is an apple, this is a pen → oh, ApplePen!*
    
    when actions resembling "word concatenation" was learned
+ */Latplan/* brings AI to a new level
  
  e.g. *1000 steps of optimal, logical reasoning over natural language*
  
  + Fundamentally impossible for short-sighted and greedy RL agents

#+begin_note
 "A hierarchical neural autoencoder for paragraphs and documents." (2015)

 "Binary coding of speech spectrograms using a deep auto-encoder." (2010)
#+end_note

** Future Work (Extended planning formalism)

+ Latplan assumes nothing about the environment machinery (grids, movable tiles...)
+ Latplan assumes *fully-observable*, *deterministic* domains
+ Next step: *Extending Latplan to MDP, POMDP*
  + Gumbel-Softmax layer → just a Softmax layer? (probability)



* Appendix

** Related Work

Compared to the work by Konidaris et al. (2014), the inputs to LatPlan are
unstructured (42x42=1764-dimensional arrays for 8-puzzle); each pixel does not
carry a meaning and the boundary between “identifiable entities” is unknown.
Also, AMA2 automatically grounds action symbols, while they rely on
human-assigned symbols (move, interact). Furthermore, they do not explicitly
deal with robustness to noisy input, while we implemented SAE as a denoising
AE. However, effects/preconditions in AMA2 is implicit in the network, and their
approach could be utilized to extract PDDL from AAE/AD (future work).

** Related Work

There is a large body of work using NNs to directly
solve combinatorial tasks, starting with the well-known TSP
solver (Hopfield and Tank 1985). Neurosolver represents a
search state as a node in NN and solved ToH (Bieszczad and
Kuchar 2015). However, they assume a symbolic input.

** Related Work

Previous work combining symbolic search and NNs embedded NNs inside a search to
provide the search control knowledge, e.g., domain-specific heuristic functions
for the sliding-tile puzzle and Rubik’s Cube (Arfaee et al. 2011), classical
planning (Satzger and Kramer 2013), or the game
of Go (Silver et al. 2016).

** Deep Reinforcement Learning (DRL)

DRL has solved complex problems, including video games where it communicates to
a simulator through images (Mnih et al. 2015, DQN). In contrast, LatPlan only
requires a set of unlabeled image pairs (transitions), and does not require a
reward function for unit-action-cost planning, nor expert solution traces
(AlphaGo), nor a simulator (DQN), nor predetermined action symbols (“hands”,
control levers/buttons).  Extending LatPlan to symbolic POMDP planning is an
interesting avenue for future work.


** I expect mixed responses such as:

#+begin_larger
+ Wait, what!? You /solved/ the symbol grounding!? (*No*)
+ Huh, I hate deep learning hype, NN cannot be trusted.
  
  (*LatPlan can be trusted*)
+ This is not an action model acquisition / not finding symbols.

  (*We don't do AML; we find different symbols*)
#+end_larger
** Did we find symbols? It doesn't sound like what I think symbols are

#+begin_smaller
You /solved/ the symbol grounding!? / This is not finding symbols. / This isn't a action model acquisition.
#+end_smaller

*PDDL implies there are several kinds of symbols and we solved only /one/ issue*

+ Each issue requires a different approach. LatPlan should be combined with these work.

  |                        | <c>                     |
  | Types of symbols       | addressed by            |
  |------------------------+-------------------------|
  | *Propositions*         | *SAE*                   |
  | Object labels          | R-CNN (Computer Vision) |
  | Predicates (relations) | ???                     |
  | Actions                | Domain Acquisition      |

** Did we find symbols? Why not individual pixels? Why DL?

*Systems based on individual pixels lack generalization*

+ Noise / variations can make the data entirely different

  [[png:results/noise]]

+ must acquire the *generalized features*

+ = a nonlinear function that recognize the entanglements between multiple pixels

** Learning vs Planning
  
 Main differences: Purposes and the abstraction layer

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 *Machine Learning, Neural Networks* 
 
 for *Recognition, Reflex*
 + *Subsymbolic Input* (continuous)
   
   Images, Audio, unstructured text: 
 + *Soft Intelligence*:
   
   　 */Reflex Agent/, /Immediate/ actions*
   #+begin_smaller
   *Pavlov's dog* : bell → drool

   *Autonomous Driving* : Pedestrian → Stop.

   *Machine Translation* : Sentence → Sentence

   *Eval. Function for Go* : board → win-rate
   #+end_smaller
   #+begin_larger
   ☺ Efficient 1-to-1 mapping
   
   ☹ Simple tasks
   #+end_larger
 #+end_span6
 #+begin_span6
 *Deliberation, Search*

 for *Planning, Game, Theorem Proving*
 + *Symbolic Input/Output*
   
   Logic, objects, induction rules
 + *Hard Intelligence by Logic:*

   　 */Multi-step/ strategies*
   
   #+begin_smaller
   *Rescue Robot* : actions → help the surviver

   *Theorem Proving* : theorems → QED

   *Compiler* : x86 instructions
   
   *Game of Go* : stones → Win
   #+end_smaller
   #+begin_larger
   ☺ Ordering constraint + complex tasks
   #+end_larger
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

+ AlphaGo = Subsymbolic (DLNN eval. function) + Symbolic (MCTS)

** Human-Competitive Systems

 AlphaGo = Subsymbolic (NN eval. func) + Symbolic (MCTS)
 + However, *domain-specific* -- specialized in Go, "Grids" / "Stones" are known
 + *Huge expert trace DB* --- Not applicable when data are scarse (e.g. *space exploration*)
 + */Is supervised learning necessary for human?/*
  
   *True intelligence should search / collect data by itself*

 DQN = Subsymbolic (DLNN) + Reinforcement Learning (DLNN)

Domain-independent Atari Game solver (Invader, Packman…), however:
 + RL Acting: Greedily follow the learned policy → *no deliberation!*
 + You can survive most Atari games *by reflex*
  
 # 実際 *Sokoban など論理思考ゲームでは性能が悪い* ↔ 倉庫番ソルバ

** Latplan Advantages

#+begin_xlarge
*/Perception/* based on DLNN
#+end_xlarge

--- Robust systems augmented by the latest DL tech

#+begin_xlarge
*/Decision Making/* based on Classical Planning
#+end_xlarge

--- *Better Theoretical Guarantee than Reinforcement Learning*

#+begin_center
*Completeness* (Finds solution whenever possible), */Solution Optimality/*
#+end_center

--- *Decision Making Independent from Learning*

#+begin_center
*/Unsupervised/* (No data required), *Explainable* (Search by logic)
#+end_center

# 今まではNNとの相性から強化学習が優勢だったが *もうその必要はない*

** When Latplan returns a /wrong/ solution?

*Machine learning may contain errors* (convergence /only on/ $t\rightarrow \infty$, not on real time)

+ Images → Fraud symbols/model/graph

+ *Optimal path on a fraud graph* or *graph disconnected*
  
  A* completeness, soundness, optimality (admissible heuristics) 

+ Fraud visualized plan (noisy) / no plan found

#+begin_center
#+begin_larger
LatPlan may make */wrong observations/* but no */wrong decisions/*
#+end_larger

BTW, "correctness" is defined by error prone observations by humans anyways ...
#+end_center

#+begin_alignright
 (completeness, optimality) → better reliablility than Reinforcement Learning
#+end_alignright

*** Reinforcement Learning

Not only *perception* but *decision making also depends on training*

+ */Each training result does not have admissibility/*

+ */When the learned policy is wrong, the solution could be suboptimal/*

#+begin_quote
... AlphaGo was unprepared for Lee Sedol’s Move 78 because it
didn’t think that a human would ever play it.

#+begin_alignright
Cade Metz. "In Two Moves, that Redifined the Future." /Wired/, 2016
#+end_alignright
#+end_quote

#+begin_center
#+begin_larger
RL may make *wrong decisions*.
#+end_larger
#+end_center

** Future Work (SAE)

SAE can generate propositional symbols (state $s = \{q,r\ldots\}$)

+ 1st-order logic (predicate $p(a,b)$ )

+ We need *object recognition from images* (parameters $a,b$)

+ SAE with networks for object recognition (e.g. R-CNN) should achieve this

** SAE implementation in LatPlan α

Keras, Adam optimizer (learning rate:0.001)

  1764(42x42)

  [→FC(4000,ReLu)→Batchnorm→Dropout(0.4)] × 2

  →FC(49,GumbelSoftmax) (variational loss)

  [→FC(4000,ReLu)→Batchnorm→Dropout(0.4)] × 2

  →1764(42x42) (loss: Binary crossentropy)

　

+ Why full-connected layers ? ::
             Main focus is on *whether propositions made by SAE are feasible*
             
             *→No complication due to CNN, ResNet, etc...* (but I have a working implementation)
             
+ Training on 8-puzzle ::
               *12000 images* out of entire states (362880) → *Generalization*

** Why symbols?

Symbols are strong abstraction mechanisms becasue

+ Meanings do not matter ::
     You do not have to understand it: Does a symbol $X$ mean an apple or a car?
 
     Logical reasoning can be performed by mechanical application of rules
     
     + Domain-independent planning : *mystery* vs *nomystery*

       Logistic domains where *symbol names are mangled* (truck → shark)

+ Composable :: 
     A latent vector is a conjunction (and)
     
     Heuristic functions use modus ponens to derive guidance

** More details

GTX1070, PhenomII X6 (3.4GHz OC), 16GB Mem

+ Training: ~30 min
+ Solving: ~3 sec

*** State AutoEncoder (Train data)

1: $x$ 2: $z$ 3: $y$ 4: $round(z)$ 5: $Decode(round(z))$

[[spng:experiment/autoencoding_train]]

*** State AutoEncoder (Validation)

1: $x$ 2: $z$ 3: $y$ 4: $round(z)$ 5: $Decode(round(z))$

[[spng:experiment/autoencoding_test]]

*** State AutoEncoder

Input 2: intial/goal images

[[spng:experiment/init_goal]]

*** PDDL Domain Definition

Examples in $N=25$ (in the paper we bypassed PDDL-SAS converter though)

#+begin_src lisp
(define (domain latent)
 (:requirements :strips :negative-preconditions)
 (:predicates (z0) (z1) (z2) (z3) (z4) (z5) (z6) (z7) (z8) (z9) (z10)
  (z11) (z12) (z13) (z14) (z15) (z16) (z17) (z18) (z19) (z20) (z21)
  (z22) (z23) (z24))
 (:action a10000010010110111100011111000010001011111110011111
  :parameters () :precondition
  (and (z0) (not (z1)) (not (z2)) (not (z3)) (not (z4)) (not (z5))
       (z6) (not (z7)) (not (z8)) (z9) (not (z10)) (z11) (z12)
       (not (z13)) (z14) (z15) (z16) (z17) (not (z18)) (not (z19))
       (not (z20)) (z21) (z22) (z23) (z24))
  :effect (and (z5) (not (z6)) (z13) (z20)))
 (:action a10000010010110111100011110000001001011011110001110
  ...
#+end_src

*** Results in one place

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6

[[png:results/mnist-plan]]

[[png:results/mandrill-plan]]

#+end_span6
#+begin_span6

[[png:results/hanoi3]]

[[png:results/hanoi4]]

[[png:results/lights-out]]

[[png:results/lights-out-skewed]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

*** Fast Downward Log Trace (Search Statistics)                    :noexport:


#+begin_example
reading input... [t=5.479e-05 (sec)]
...
Building successor generator...done! [t=31.7737 (sec)]
done initalizing global data [t=31.7738 (sec)]
Conducting best first search with reopening closed nodes, (real) bound = 2147483647
Initializing landmark cut heuristic...
f = 4 [1 evaluated, 0 expanded, t=36.1569 (sec), 1054016 KB]
...
f = 12 [339 evaluated, 183 expanded, t=843.001 (sec), 1054016 KB]
f = 13 [553 evaluated, 314 expanded, t=1357.62 (sec), 1054016 KB]
f = 14 [942 evaluated, 529 expanded, t=2288.7 (sec), 1054016 KB]
Solution found!
Actual search time: 2688.73 (sec) [t=2724.89 (sec)]
a11010000010010000101001111101100010001000010100111 (1)
...
a10011100111110010010011011001111010111011001001111 (1)
a10011110101110110010011111001011001111011001001111 (1)
Plan length: 14 step(s).
Expanded 631 state(s).
Evaluated 1140 state(s).
Generated 1924 state(s).
Dead ends: 24 state(s).
Search time: 2693.06 (sec)
Total time: 2724.89 (sec)
#+end_example

** Does it have something to do with symbolic planning (BDDs) ?

No.

