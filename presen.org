#+title: Running a GOFAI in Deep Latent Space: Bridging the Sybsymbolic-Symbolic Boundary
#+include: "head.org"
#+LINK: img file:img/%s
#+LINK: png file:img/%s.png
#+LINK: jpg file:img/%s.jpg
#+LINK: svg file:img/%s.svg
#+LINK: spng file:img/static/%s.png
#+LINK: sjpg file:img/static/%s.jpg
#+LINK: ssvg file:img/static/%s.svg
#+LINK: sgif file:img/static/%s.gif

#+begin_outline-text-1
#+begin_center

#+begin_larger
Masataro Asai

The University of Tokyo
#+end_larger

20+ min
#+end_center

#+begin_note
#+begin_alignright
Made by guicho2.71828 (Masataro Asai)
#+end_alignright
#+end_note
#+end_outline-text-1

* Introduction 

#+begin_xlarge
#+begin_center
Hello, Machine Learning people!
#+end_center
#+end_xlarge

#+begin_center
Deep Learning, RL, data mining, etc...

#+begin_xlarge
+ Do you identify yourself as a connectionist? ‚úã
#+end_xlarge
#+end_center

#+begin_resume
First of all, hello machine learning people.
Im not a machine learning person.
Im usually studying a type of symbolic AI and I visited here expecting an interesting conversation with
machine learning professionals.
#+end_resume

** Introduction 

#+begin_xlarge
#+begin_center
Hello, Symbolic AI people!
#+end_center
#+end_xlarge

#+begin_center
SAT, CP, Logic, TCS, ...

#+begin_xlarge
+ Have you tried any DL libraries? ‚úã
#+end_xlarge
#+end_center

#+begin_resume
And also, hello other symbolic AI people.
Although I am not doing exactly the same thing, we have common roots.

Let me give you one question before presenting my paper...
#+end_resume

** Introduction 


#+begin_larger
#+begin_center
*I am from /AI Planning/ community (symbolic, logic-based)*
#+end_center
#+end_larger

#+begin_center
[[sgif:icapslogo]]

International Conference on Automated Planning and Scheduling

(AAAI sister conference, 33% avg. accept ratio, since 1990, ECP+AIPS‚ÜíICAPS)
#+end_center

** We are working on */Automated Planners/*

„ÄÄ

#+begin_center
#+begin_larger
*High-level plan for agents*

/plan = (action_1, action_2, ... action_n)/

Initial state ‚Üí üö∂  ‚Üí üöó  ‚Üí üî® ‚Üí Goal state
#+end_larger
#+end_center

„ÄÄ

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span4
*1st-Order Logic*

STRIPS/PDDL

modelling language

Close-world assumption
#+end_span4
#+begin_span4
*Combinatorial Explosion*

High-dimensional

implicit state space

PSPACE-Hard
#+end_span4
#+begin_span4
*Expensive, Critical Applications*

Satellite Operation

Deep Space 1

Mars Exploration Rover
#+end_span4
#+end_row-fluid
#+end_container-fluid
#+end_center

„ÄÄ

+ 
  #+begin_center
  #+begin_larger
  *Soundness, Completeness, Optimality*

  of the algorithms are important
  #+end_larger
  (compared to "planning" in other AI fields)
  #+end_center


# + Compute the *high-level plan* for an agent: /plan = (action_1, action_2, ... action_n)/
#   + Search-based planning projects many steps further into the future than RL
#   + ‚Üî reflex (0-step), limited deliberation (1,2-steps)
# + High-dimensional, *implicit* state space search
#   + Entire space does not fit in memory, only *transition rules* are given
# + STRIPS-like *PDDL modelling language*, based on
#   + 1st-order logic, close-world assumption
# + *Soundness, completeness, optimality* matters
#   + Expensive Applications: Satellite operations (Deep Space 1), Mars Exploration Rover (MER), underwater vehicles

** Introduction 

#+begin_xlarge
#+begin_center
Today, I talk about how to integrate DL and logic-based planning systems.
#+end_center
#+end_xlarge

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*/Backgrounds/*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Sliding tile puzzle (a.k.a 8-puzzle)

#+begin_container-fluid
#+begin_center
#+begin_row-fluid
#+begin_span4
[[png:8puzzle-standard]]
Initial State
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
Goal State
#+end_span4
#+begin_span4
[[sgif:8puzzle]]
#+end_span4
#+end_row-fluid
#+end_center
#+end_container-fluid

#+begin_alignright
+ *You need to find a plan to reach the goal.*
#+end_alignright

** States

*States* as *1st-order logic formula*, described in *PDDL language*.

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
#+begin_quote
#+begin_smaller
/Empty(x_0, y_0)/

/Is(panel_6, x_1, y_0)/

/Up(y_0, y_1), Down(y_1, y_0).../

/Right(x_0, x_1), Left(x_0, x_1).../
#+end_smaller
#+end_quote
#+end_span6
#+begin_span6
#+begin_src lisp
(empty x0 y0)
(is panel6 x1 y0)
(up    y0 y1), (down y1 y0)...
(right x0 x1), (left x0 x1)...
#+end_src
#+end_span6
#+end_row-fluid
#+begin_center
#+begin_row-fluid
#+begin_span4
[[png:8puzzle-standard]]
Initial State
#+end_span4
#+begin_span4
[[png:8puzzle-standard-goal]]
Goal State
#+end_span4
#+begin_span4
[[sgif:8puzzle]]
#+end_span4
#+end_row-fluid
#+end_center
#+end_container-fluid

** Transitions / Actions

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
Representing  *sliding up* ‚Üí *Transition Rules (Actions)*

#+begin_quote
+ *When* the new location is empty:

+ The current location *becomes empty*.

+ New location is *occupied* and is *not empty*.
#+end_quote
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+begin_row-fluid
#+begin_span12
+ *First-order Logic Formula*
  
  *When* /Empty(x, y_{old}) ‚àß is(panel, x, y_{new}) ‚àß up(y_{new}, y_{old})/ ;
  
  *then* /¬¨ Empty(x,y_{old}) ‚àß Empty(x,y_{new}) ‚àß ¬¨ is(panel, x, y_{new}) .../

+ *PDDL Model* : Actual Input to the Planner
  #+begin_src lisp
  (:action slide-up ...
   :precondition (and (empty ?x ?y-old) ...)
   :effects (and (not (empty ?x ?y-old)) (empty ?x ?y-new) ...))
  #+end_src

# #+begin_src lisp
# (:action move-up
#  :parameters (?x ?y-old ?y-new ?panel)
#  :precondition (and (empty ?x ?y-old)
#                     (up ?y-old ?y-new)
#                     (is ?panel ?x ?y-new))
#  :effects (and (not (empty ?x ?y-old))
#                (empty ?x ?y-new)
#                (not (is ?panel ?x ?y-new))
#                (is ?panel ?x ?y-old)))
# #+end_src
#+end_span12
#+end_row-fluid
#+end_container-fluid

# + 
#   #+begin_larger
#   #+begin_alignright
#   But *where does this representation come from?*
#   #+end_alignright
#   #+end_larger

** Symbolic planners cannot solve an */Image-based/* 8-puzzle

#+begin_center
+ *PDDL for 3x3 Sliding Tile Puzzle* can be solved *optimally* *<< 1sec*
#+end_center

#+begin_container-fluid
#+begin_row-fluid
#+begin_span8
[[sjpg:puzzle]]
#+end_span8
#+begin_span4
+ 
   #+begin_center
   *BUT*

   *WE*

   *DO*

   *NOT*

   *HAVE*

   *A*

   #+begin_larger
   */PDDL/*

   */MODEL/!*
   #+end_larger
#+end_center
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Knowledge-Acquisition Bottleneck (Cullen, 1988):

#+begin_quote
The *cost of human* involved for converting *real-world problems* into inputs for
domain-independent *symbolic* systems
#+end_quote

#+begin_container-fluid
#+begin_row-fluid
#+begin_span12
+ *For /image-based/ tasks*, we must *automate 2 processes*:
#+end_span12
#+end_row-fluid
#+begin_row-fluid
#+begin_span6
+ *1. Symbols Grounding:*

  #+begin_center
  #+begin_larger
  */Symbols/ = identifiable entities*
  #+end_larger
  #+end_center
  
  #+begin_smaller
  | Types        | Examples                     |
  |--------------+------------------------------|
  | Objects      | *panel7*, *x_0*, *y_0* ...   |
  | Predicates   | (*empty* ?x ?y)              |
  | Propositions | *p_28* = (empty x_0 y_0)     |
  | Actions      | (*slide-up* panel_7 x_0 y_1) |
  #+end_smaller
#+end_span6
#+begin_span6
+ *2. Action Model Acquisition (AMA):*
  
  #+begin_center
  #+begin_larger
  */Describe/ actions.*
  #+end_larger
  #+end_center
  
  „ÄÄ

  #+begin_center
  *When* /Empty(x, y_{old}) ‚àß .../ ;

  *Then* /¬¨Empty(x,y_{old}) ‚àß/ ...
  #+end_center

#+begin_center
+ A *Long-Standing Problem*...

+ 
  #+begin_larger
  *Now we propose a system which automate these processes...*
  #+end_larger
#+end_center

#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_note
# The knowledge acquisition bottleneck: time for reassessment? : Cullen, J and Bryman, A Expert Syst. Vol 5 No 3 (August 1988) pp 216-225
# #+end_note

* Latent-Space Planner (*/Latplan/*) (*/Accepted/* in AAAI-2018)

[[png:latplanlogo]]

* Survey of Exisiting Action Model Acquisition Techniques          :noexport:

#+begin_xlarge
Survey of Exisiting Action Model Acquisition Techniques
#+end_xlarge

„ÄÄ

#+begin_alignright
i.e. Systems that find action models
#+end_alignright

** Limitations of Existing Systems                                 :noexport:

#+begin_xlarge
#+begin_center
So far, ALL existing AMA systems require */symbolic / near-symbolic, accurate state inputs/* and/or */discrete action labels/*.
#+end_center
#+end_xlarge

#+begin_alignright
i.e. They need symbols to find an action model
#+end_alignright

** So far, ALL existing AMA systems require */symbolic inputs/*

ARMS (Yang AIJ07)
LOCM (ICAPS09)
Argall (AIJ09)
Mourao (UAI12)

All taking the *symbolic* inputs to find the *action models*

[[spng:locm]]

** Framer (ICAPS17)                                                :noexport:

*Near-Symbols* : Parses natural language sentences with a *clear grammatical structure*.


#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[spng:framer]]
#+end_span6
#+begin_span6
+ Alleviates the burden of domain experts, but *still requires human*
+ Not handling "Natural Language":
  
  #+begin_quote
  Pick up that parcel over there ... yeah, it has a label on it, it says Parcel1, you can see
  it from here, the Location B. Then put it in the car, I mean the truck, the red one.
  #+end_quote
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Konidaris, Kaelbring (AAAI14, IJCAI15)                          :noexport:

"Constructing Symbolic Representations for High-Level Planning" (AAAI14)

+ What it does :: Converting a *Semi-MDP Model* to a *PDDL Model* by set-theoretic representation
                  
                  i.e. *Model-to-Model* conversion, not *generating a model from the scratch*
+ Semi-MDP contains Action Labels :: =move= and =interact= (Playroom)
+ Sensor inputs are structured (Labels for "State Variable" are known) :: 

     x/y-distance, light level, whether a monkey cries
     
     ‚Üí Each sensor has a distinct meaning (no overwrap)

# + Low-dimensional, accurate input :: 33 vars (Playroom), 9 vars (Treasure), no noise
#      
#      Although IJCAI15 shows "visual depiction", it is not used by the system

** Learning from Video for Board Game (Bardu ICRA10; Kaiser AAAI12; Kirk 16) :noexport:

*Handles Images, but with strong assumptions (almost symbol)* e.g.

#+begin_quote
Tic-Tac-Toe with *Ellipse Detectors* (Bardu 10)
     
‚Üí Almost immediately provides propositions

‚Üí Also, Domain-dependent ("3x3 grid" "Ellipse" are hard-coded)
#+end_quote

* Problem Setting

#+begin_center
#+begin_xlarge
Problem Setting

of

Latent-Space Planner
#+end_xlarge
#+end_center

** Task: Solving an Imaged-Based 8-puzzle w/o Prior Explicit Knowledge

*No Prior Knowledge* : labels/symbols such as "9 tiles", "moving"

[[sjpg:puzzle]]

** Task: Solving an Imaged-Based 8-puzzle */w/o Prior Explicit Knowledge/*

*/No Prior Knowledge/* : labels/symbols such as "9 tiles", "moving"

[[sjpg:puzzle]]

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

*/No Prior Knowledge/* : */Domain-independent Image-based planner/*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
Tower of Hanoi

[[sjpg:hanoi]]
#+end_span6
#+begin_span4
Lights-Out

[[sjpg:lightsout]]
#+end_span4
#+end_row-fluid
#+end_container-fluid

** Inputs

#+begin_xlarge
The system is given *2 types of input*:

+ Training Input
+ Planning Input
#+end_xlarge

** Input1: Training Input -- Image Pairs

[[png:overview/1]]

** Input1: Training Input -- Image Pairs

#+begin_right
[[png:overview/2]]
#+end_right

They are *random sampled* transitions

+ No *reward* / No *expert traces*
+ No *access to the simulator*
+ 
  #+begin_larger
  No */action labels, symbols/*

  „ÄÄ„ÄÄ(e.g. up, down)
  
  ‚Üí doesn't know what's happening in each transition
  #+end_larger

** Input2: Planning Input -- Initial Image & Goal Image

[[png:overview/input2]]

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

#+HTML: <embed src="img/overview/3.svg" type="image/svg+xml"  />

** Task: Solving */ANY/* Imaged-Based tasks w/o Prior Explicit Knowledge

#+HTML: <embed src="img/overview/3-hanoi.svg" type="image/svg+xml"  />

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*/Latplan Architecture/*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Latent-Space Planner (*/LatPlan/*) architechture

 [[png:overview/planning1]]

** Step 1: Propositional Symbol Grounding

 [[png:overview/planning2]]

*** Step 1: State Autoencoder                                      :noexport:

[[png:train-state-ae]]

Trained SAE provides two functions:

+ $b = Encode(r)$ *maps a raw datum $r\;$ to a bit vector $b\;$*

+ $\tilde{r} = Decode(b)$ *maps a bit vector $b\;$ to a raw datum $\tilde{r}$*

** Step 2: Action Model Acquisition (AMA)

 [[png:overview/planning3]]

** Step 3: Solve the Symbolic Planning Problem

  [[png:overview/planning4]]

** Step 4: Executing the Symbolic Plan

  [[png:overview/planning5]]

** Step 5: Obtaining the Visualization

  [[png:overview/planning6]]

#+begin_center
#+begin_larger
#+end_larger
#+end_center

** Summary

#+begin_xlarge
Latplan: Latent-space Planner.
#+end_xlarge

#+begin_larger
+ *bridges /real-world/ and /propositional/ representations.*
+ *performs a /propositional, logical, sound reasoning/.*
+ *returns a /real-world/ output* (e.g. images).
+ *runs in a /completely automated, unsupervised/ manner.*
#+end_larger

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*/State AutoEncoder (SAE)/*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* State AutoEncoder (SAE)

SAE is a *neural network* which provides two functions:

+ $b = Encode(r)$ : *maps a raw datum $r\;$ to a bit vector $b\;$*

+ $\tilde{r} = Decode(b)$ : *maps a bit vector $b\;$ to a raw datum $\tilde{r}$*

#+begin_larger
+ *A bidirectional mapping between*

  + *subsymbolic representation* (image array) and

  + *symbolic representation*
 
    #+begin_alignright
     (bit vectors = propositional variables)
    #+end_alignright
#+end_larger

** Neural Network 101                                              :noexport:

[[png:deeplearning/1]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/2]]

** Neural Network 101                                              :noexport:

[[png:deeplearning/3]]

** Stochastic Gradient Descent + GPU                               :noexport:

[[spng:gradient-descent]]

Plus misc techniques e.g. *Batchnorm*, *Dropout*

#+begin_larger
*Pretty much everything is on the standard online tutorial / lecture cource / MOOP*

*Good libraries --- Tensorflow, Keras* --- you can learn in 1-2 months
#+end_larger

** NN for Standard Classification Tasks (Supervised)

# Target Function $y=f(x)\;$ trained by SGD minimizing $|y-f(x)|$
# 
# #+begin_alignright
# SGD: Stochastic Gradient Descent
# #+end_alignright
  
| Task                 | Input x  | Output y                           |
|----------------------+----------+------------------------------------|
| Image classification | Image    | Label (1=car, 2=cat, 3=monkey ...) |
# | Translation          | Sentence | Sentence                           |
# | Go eval. function    | State    | Number                             |

#+begin_larger
 + *This is not suitable for our task;*
   + */There are no labels/* provided by humans, i.e.
   + */There are no real answers/* for symbol grounding
     
     „ÄÄ

     #+begin_alignright
     (*People do it differently, too.*
     
     cf. How many colors in a üåà?)
     #+end_alignright

#+end_larger

** Unsupervised Learning by AutoEncoder

# Auto = "self" --- Autoencoding = "encoding itself"

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
„ÄÄ

Target Function: Identity $x=f(x)$
+ Encode $x\;$ to a *latent vector* $z$
+ Decode $z\;$ back to the input $x$
+ $z\;$ has a smaller dimension
+ i.e. Compression: $X \leftrightarrow Z$
+ Training: Minimize $|x - f(x)|\;$
  (*reconstruction loss*)
#+end_span7
#+begin_span5
[[png:deeplearning/autoenc]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
#+begin_larger
+ ‚Üí However, */‚úò Latent vector Z is real-valued/*

  */INCOMPATIBLE to propositional reasoning/*
#+end_larger
#+end_alignright

** Variational AutoEncoder (VAE)                                   :noexport:

An AutoEncoder that *enforce a certain distribution* on $Z \subset \mathbb{R}^n$ over the dataset $X$

#+begin_quote
You have $X=$ { 10k images of apples }. If you train a *Gaussian VAE* on $X$, then $Z = Encode(X) \approx N(\mu,\sigma)$ for some $\mu,\sigma \in \mathbb{R}^n$.
#+end_quote

VAE needs a *reparametrization trick* because random distributions are non-differentiable.

#+begin_quote
Reparametrization for $N(\mu,\sigma)$: $\mu + \sigma N(0,1)$

#+begin_center
\mu and \sigma are differentiable vectors, $N(0,1)$ is not.
#+end_center
#+end_quote

** Gumbel-Softmax VAE (Jang, Gu, ICLR2017)

*Additional optimization penalty* which enforces $Z \sim \textbf{Categorical}$:

„ÄÄ„ÄÄ„ÄÄ‚Üí $z\;$ converges to a 1-hot vector e.g.  $\langle 0,0,1,0 \rangle$ .

Example: Represent an MNIST image with 30 variables of 8 categories.

#+begin_center
 #+begin_html
 <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="649px" height="206px" version="1.1" content="&lt;mxfile userAgent=&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36&quot; version=&quot;6.0.1.2&quot; editor=&quot;www.draw.io&quot; type=&quot;google&quot;&gt;&lt;diagram name=&quot;Page-1&quot;&gt;3ZhLc5swEMc/Dcd0kAQCX+O67SGd6TTTaXqU0fJoZcsjy69++oogDBjs0MZ2TONDpL9ey29Xi5BDxrPtR8UW6WfJQTjY5VuHvHcwxshF5l+u7AqF+rgQEpXxQkKV8Jj9Biu6Vl1lHJaNjlpKobNFU4zkfA6RbmhMKblpdoulaK66YAm0hMeIibb6PeM6LdTQdyv9E2RJWq6MXNsyY2VnKyxTxuWmJpGJQ8ZKSl2UZtsxiBxeyaUY9+FI694wBXPdZ0AQh1EYx4xP46kXRHBnZ1gzsbIPaw3Vu/LpN2mm4XHBory+MR52yH2qZ8LUkCnGmRBjKaR67k0AcR8Coy+1kr+g1jKiAWHUtCi5mnPgdrw1AJSG7dGnQntWJshAzkCrneliB5CAFENsfCHPL+qbylt7n6Q1T+HAisxGSLKfu4JoCpZjT6a4xRT71MFUmFXvebY2xSQvfoWHb6VsFqm1tHygUjmbroyR9y944ww08QFN7LktmmEHzPASLEmLpY/wcFiiW2LptVAAN3nOVqXSqUzknIlJpdb2qtuEA9tMP9XKP/Iu7/y8NjeGPtkRz5Wq7SdovbMJnq20NFK17oOUiwb63LzT4M3TyJWK4HT0aKYS0Kd2a9uBCgTT2bq5/mvc4eEIU4xG4PsesCDqCO1z+ufKpBHugZq8FeorRf6VmfdBfiQ9XR45HXbi9kf4zRJ3i2Uw7AOFd0ssw//9JXgseupponu3Ht8MwYEDL+KZ0cDehyeBBqd3BB31B2pn+SIzY0D/KYqYsKMO3LK36N88hc7+1cgZhHHU9dVIoxCm8SW+GukINQm67bSEaEde2otnDX901nAfRGaiPTYSutYJ5uiRdthRfldeot1IlPfI8i9Abcb8AeLYz3+diJ//Xgm1hIg6IHpdEL1LQCxvEwcK0Q4I3IP8+8ZQX/9SuwGoJGxud+STFtSQtpli3/trpqZa3VwX54rq/p9M/gA=&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g transform="translate(0.5,0.5)"><rect x="288" y="0.75" width="75" height="202.5" rx="11.25" ry="11.25" fill="#e1d5e7" stroke="#9673a6" pointer-events="none"/><path d="M 243 72 L 273 102 L 243 132 L 213 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(231.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 168 72 L 198 102 L 168 132 L 138 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(156.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 198 102 L 208.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 212.16 102 L 206.91 104.63 L 208.22 102 L 206.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 120.75 102 L 135.75 102 L 123 102 L 133.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 137.16 102 L 131.91 104.63 L 133.22 102 L 131.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 273 102 L 288 102 L 273 102 L 283.22 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 287.16 102 L 281.91 104.63 L 283.22 102 L 281.91 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 482.25 72 L 512.25 102 L 482.25 132 L 452.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(470.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">512<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 407.25 72 L 437.25 102 L 407.25 132 L 377.25 102 Z" fill="#ffffff" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><g transform="translate(395.5,91.5)scale(0.75)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="26" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; word-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;">256<div>ReLU</div></div></div></foreignObject><text x="15" y="19" fill="#000000" text-anchor="middle" font-size="12px" font-family="Helvetica">[Not supported by viewer]</text></switch></g><path d="M 437.25 102 L 447.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 451.41 102 L 446.16 104.63 L 447.47 102 L 446.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 360 102 L 375 102 L 362.25 102 L 372.47 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 376.41 102 L 371.16 104.63 L 372.47 102 L 371.16 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="526.5" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><path d="M 512.25 102 L 521.72 102" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 525.66 102 L 520.41 104.63 L 521.72 102 L 520.41 99.38 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="0.75" y="42" width="120" height="120" rx="18" ry="18" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/>
 <image xlink:href="img/static/x0.gif" x="8.25" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/x1.gif" x="534" y="49.5" width="105" height="105" fill="#f5f5f5" stroke="#666666" pointer-events="none"/>
 <image xlink:href="img/static/y.gif" x="293.25" y="6.75" width="64.5" height="190.5" fill="#f5f5f5" stroke="#666666" pointer-events="none"/></g></svg>
 #+end_html
#+end_center

#+begin_center
+ Key idea: *These categorical variables are /directly/ usable*

  *as the source of propositional models*

  In particular, *2 categories ‚Üí propositional variables (true/false)*
#+end_center

** State Autoencoder (*/before training/*)

 [[png:sae/state-ae-before]]

** State Autoencoder (_/after training/_)

 [[png:sae/state-ae]]

** Gumbel-Softmax: Differential Approximation of Gumbel-Max        :noexport:

#+begin_larger
It uses *annealing* to approximate discrete vectors
#+end_larger

#+begin_smaller
Gumbel-Max: Method for drawing one-hot vector sample ($z$) from category probability ($x$)

+ E.g.: $x=[0.1, 0.1, 0.8] \rightarrow z = [1,0,0] \text{or} [0,1,0] \text{or} [0,0,1]$

+ $z = \text{ GumbelMax}(x) = [ i == \arg \max_j (\text{ Gumbel}(0,1)+\log x_j) \; ? \; 1 : 0 ]$

+ argmax is non-differentiable ‚Üí softmax approximation (differentiable)

+ $z = \text{ GumbelSoftmax}_\tau (x) = \text{ Softmax}( [\text{ Gumbel}(0,1)+\log x_j]/\tau )$

+ Temparature $\tau \rightarrow 0$ , $z\rightarrow \text{one-hot vector}$

  # \[
  # \text{ GumbelSoftmax}_\tau (x) \rightarrow \text{ GumbelMax}(x) \quad (\tau\rightarrow 0)
  # \]
#+end_smaller

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span8
#+begin_center
[[png:sae/gumbel]]
#+end_center
#+end_span8
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

#+begin_note
Maddison et. al., 2014
#+end_note

* Verifying the feasibility of Latplan system and SAE

#+begin_quote
Q: Do the *propositions* found by the SAE (neural network) *make any logical sense?*

Q: Is it possible to *reason over such generated propositions?*
#+end_quote

We need to build a *MVP system*

„ÄÄ

#+begin_center
*SAE is not sufficient for propositional reasoning:* We need *AMA*
#+end_center

„ÄÄ

#+begin_alignright
#+begin_larger
So we built *AMA ver.1*, a */trivial, oracular/* AMA system
#+end_larger
#+end_alignright

** AMA_1 : a */trivial, oracular/* AMA method

It does not actually *learn/generalize* anything

„ÄÄ

 [[png:overview/ama1]]

** Embed Directly: One Action Per Transition

#+begin_src lisp
 0011 ‚Üí 0101  

„ÄÄ„ÄÄ„ÄÄ‚Üì       ;; one action per transition
(:action       action-0011-0101
               ;; before-state is embedded directly
 :precondition (and (b0-false) (b1-false) (b2-true) (b3-true))
               ;; effect = state diff
 :effect       (and (not (b1-false)) (b1-true)
                    (not (b2-true))  (b2-false)))
#+end_src

Conversion from a bit to a proposition:

#+begin_center
 $i$-th bit is 1 ‚Üí Proposition ($b_i$ -true)

 $i$-th bit is 0 ‚Üí Proposition ($b_i$ -false)
#+end_center

*** Example PDDL

# Examples in $N=25$ (in the paper we bypassed PDDL-SAS translater)

#+begin_src lisp
(define (domain latent)
 (:requirements :strips)
 (:predicates (b0-true) (b0-false) (b1-true) ... (b24-false))

 (:action a10000010010110111100011111000010001011111110011111
  :parameters () :precondition
  (and (b0-true) (b1-false) (b2-false) ... (b24-true))
  :effect (and (not (b5-false))  (b5-true)
               (not (b6-true))   (b6-false)
               (not (b13-false)) (b13-true)
               (not (b20-false)) (b20-true)))

 (:action a10000010010110111100011110000001001011011110001110
  ...
#+end_src

** Step 3: Solve the Symbolic Planning Problem

  [[png:overview/planning4]]

** Step 4: Executing the Symbolic Plan

  [[png:overview/planning5]]

** Step 5: Obtaining the Visualization

  [[png:overview/planning6]]

* AMA_1 Experiments

SAE: *trained with 20k images* (note: > 360k entire states in 8-puzzle)

+ */SAE is generalizing/*

AMA_1: requires the entire transistions (note: > 1M transitions in 8-puzzle)

+ _/AMA_1 is NOT generalizing, being an oracle/_

Planner: a State-of-the-Art, Fast Downward (Helmert, 08)

+ $A^*$ *(optimal search)* : *It must find an optimal solution*

+ Runtime: ~3sec (instances are too small for symbolic systems)

** 8-puzzle Results with MNIST tiles (MNIST 8-puzzle)

 8-puzzle using digits from MNIST database

 [[png:results/mnist-plan]]

 #+begin_larger
 An instance whose optimal solution length is known
 #+begin_xlarge
 #+begin_alignright
  ‚Üí *31 step optimal plan*
 #+end_alignright
 #+end_xlarge
 #+end_larger

** Results with photographic, unseparated tiles (Mandrill 8-puzzle)

MNIST 8-puzzle has cleanly separated objects -> This domain does not.

[[png:results/mandrill-intro]]

** Results with photographic, unseparated tiles (Mandrill 8-puzzle)

[[png:results/mandrill-plan]]

#+begin_xlarge
#+begin_alignright
 ‚Üí *Optimal Solution*
#+end_alignright
#+end_xlarge

** Results with photographic, unseparated tiles (Spider 8-puzzle)

[[png:results/spider-plan-new]]

#+begin_center
#+begin_larger
Notice that *latplan has no idea that this is an 8-puzzle*; Thus MNIST, Mandrill, Spider are entirely different domains (state encoding is also different)
#+end_larger
#+end_center

** Tower of Hanoi (3 disks, 4 disks)

Completely different puzzle problems can be solved by the same system

[[png:results/hanoi3]]

[[png:results/hanoi4]]

#+begin_alignright
#+begin_xlarge
 ‚Üí *Optimal Solution* (7 steps,15 steps)
#+end_xlarge
#+end_alignright

** Lights Out

Completely different puzzle problems can be solved by the same system

[[png:results/lightsout_new4x4]]

#+begin_alignright
#+begin_xlarge
 ‚Üí *Optimal Solution*
#+end_xlarge
#+end_alignright

** Twisted Lights Out

Does not assume grid-like structures

[[png:results/lightsout_twisted_new4x4]]

#+begin_alignright
#+begin_xlarge
 ‚Üí *Optimal Solution*
#+end_xlarge
#+end_alignright

** Handling the Noisy Input

SAE implementation uses a denoising layer (Denoising AE)
# Denoising AE „Çí‰Ωø„Å£„Å¶„ÅÑ„Çã„Åü„ÇÅÂÖ•Âäõ„Éé„Ç§„Ç∫„Å´Â∑¶Âè≥„Åï„Çå„Åö„Å´„Éó„É©„É≥„ÇíÊ±Ç„ÇÅ„Çâ„Çå„Çã

[[png:results/noise-new]]

#+begin_larger
#+begin_alignright
 ‚Üí *Benefit from existing DL methods immediately*

 ‚Üí *Improved speed, robustness, accuracy*
#+end_alignright
#+end_larger
* Why bother the off-the-shelf planner? Shouldn't the blind search do?

*Domain-independent lowerbounds works, /SURPRISINGLY!/*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ This is *NOT* a trivial finding!

  lower-bounds are...

  + taylored for *man-made* domains
  
  + assumes the domain has a *structure*

  Blind search even sometimes outperform sophisticated methods on man-made instances (Edelkamp 12)

+ lb works ‚Üí *the more difficult problems can be solved (future work)*
#+end_span6
#+begin_span6
#+begin_smaller

| domain          | Dijkstra   | A*+PDB          | instances |
|-----------------+------------+-----------------+-----------|
| MNIST           | 210k(50k)  | *97k* (53k)     |        13 |
| Mandrill        | 176k(13k)  | *112k* (57k)    |        12 |
| Spider          | 275k(65k)  | *58k* (30k)     |        13 |
| Hanoi (4 disk)  | 20.7(29.7) | *12.6* (22.0)   |        30 |
| LightsOut (4x4) | 433(610.4) | *130.3* (164.4) |        16 |
| Twisted   (4x4) | 398(683.1) | *29.3* (62.4)   |        16 |
|-----------------+------------+-----------------+-----------|

#+end_smaller
#+end_span6
#+end_row-fluid
#+end_container-fluid

* Why Gumbel-Softmax is necessary?

+ Alternative 1: Use a normal NN and *round* the encoder output?
  + ‚úò The *decoder* is not trained with 0/1 value
+ Alternative 2: Include the *round* operation in a NN?
  + ‚úò Rounding is *non-differentiable* / *Backpropagation impossible*

* State Autoencoder Conclusion

+ SAE can learn from small examples ::

     20k training images ‚Üí learn to map 360k unseen images

     (AMA_1 is trivial, though)

+ SAE-based propositions are sound ::

     Planners can reason over the generated propositions

+ Latplan maintains the theoretical guarantee in the search algorithm ::

     Given the complete state space graph,

  + Optimising algorithm (A*) returns an optimal solution

  + Completeness is guaranteed

* *** ‚òï Break ‚òï ***

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

*/‚òï Break ‚òï/*

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*/AMA_2 Overview/*

*Action AutoEncoder (AAE)*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* SAE Feasible! Now what?

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
*AMA_1 is impractical*, cannot obtain *entire data* in the real world

#+begin_center
+ *AMA should* 

  *learn / generalize*

  *from small examples*
#+end_center
#+end_span6
#+begin_span6
 [[png:overview/ama1]]
#+end_span6
#+end_row-fluid
#+end_container-fluid

#+begin_larger
#+begin_alignright
+ ‚Üí *AMA_2*, a novel neural architecture
#+end_alignright
#+end_larger

* The Task of AMA_2 : The */Real/* AMA method

*Input:* Propositional transitions $\{ (s,t) \ldots \}$

#+begin_alignright
($Encod$'ed from Training Input)
#+end_alignright

+ *Action Symbol Grounding*
  
  #+begin_src lisp
  (:action slide-up-tile7-at-x0-y1 ...
  #+end_src
+ *Action Preconditon Learning*
  
  #+begin_src lisp
   :precondition (and (empty x0 y0) ...)
  #+end_src
+ *Action Effect Learning*
  
  #+begin_src lisp
   :effects      (and (empty x0 y1) (not ...)))
  #+end_src

** Action Symbol Grounding                                         :noexport:

Identifing a *type* of transition (clustering)

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition ...
 :effects      ...)
 #+end_src
 #+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

# + E.g. *slide-up-tile7-at-x0-y1* action abstracts all transitions that looks like this
# 
# + ‚Üí not merely an individual transition

** Action Precondition Learning                                    :noexport:

Learns *when* that transition is allowed

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition (and (empty x0 y0) ...)
 :effects      ...)
#+end_src
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

** Action Effect Learning                                          :noexport:

Learns *what* happens after the transition

#+begin_container-fluid
#+begin_row-fluid
#+begin_span9
#+begin_src lisp
(:action slide-up-tile7-at-x0-y1 ...
 :precondition (and (empty x0 y0) ...)
 :effects      (and (empty x0 y1) (not ...)))
#+end_src
#+end_span9
#+begin_span3
initial state
[[png:8puzzle-standard-tile7]]
#+end_span3
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
+ AMA_2 perform all three processes.
#+end_alignright

** Why */action symbols/* are necessary?

 Planners typically perform a *forward search* *(Dijkstra, $A^Ôºä$ )*

 Without Action Symbols, *successor generation* becomes challenging

 + SAE with $|z|=36 \text{bit}$

   ‚Üí Generate *$2^{36}$ potential successors*, then filter the invalid

 + */With action symbols/, (e.g. up, down, left, right)*

 + ‚Üí We can *enumerate the candidates in /constant time/*.

** The challenge

*Training Input lacks Action Symbols, just pairs of images*

[[png:ama/action-symbol]]

+ ‚Üí Doesn't know *which transitions are of the same type*

** Precondition/Effect learning is not trivial

#+begin_xlarge
Case 1: Linear Space
#+end_xlarge

+ *We do not need an action label*; it is linear anyways
+ Then, AMA ‚â° *prediction task* ($\approx$ scene prediction in video)
  + We can train a neural network $a(s) = t\;$ by minimizing $|t-a(s)|$

[[png:ama/linear]]

** Precondition/Effect learning is not trivial

#+begin_xlarge
Case 2: Graphs
#+end_xlarge

#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
#+begin_center
+ *Multiple action labels*
+ */Varying/ number of successors*

  */for each state/*

+ *Cannot train $a(s) = t\;$ for each $a$*
  because *we don't know which (s,t) belongs to which $a$.*
#+end_center

#+end_span7
#+begin_span5
[[png:ama/non-linear]]
#+end_span5
#+end_row-fluid
#+end_container-fluid

#+begin_alignright
#+begin_larger
+ *Correct Question*: *What* is the right function to learn?
#+end_larger
#+end_alignright

* AMA_2 Overview

[[png:ama/overview0]]

** AMA_2 Overview

 [[png:ama/overview1]]

** AMA_2 Overview

 [[png:ama/overview1-1]]

** AMA_2 Overview

 [[png:ama/overview2]]

** AMA_2 Overview

 [[png:ama/overview3]]
* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*
 
*/Action AutoEncoder (AAE)/*

*Action Discriminator (AD)*
#+end_xlarge
#+end_center

* Action AutoEncoder

*What* is the right function to learn?

#+begin_center
#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
+ $a(s) = t$ ?
+ $a$ is actually a variable!
#+end_span6
#+begin_span6
+ $apply(a,s) = t$
+ Transition is a mapping $a \rightarrow t$

  *conditioned by $s$*
  
  (previous state)

+ *The right function to learn!*
#+end_span6
#+end_row-fluid
#+end_container-fluid
#+end_center
* Action AutoEncoder

[[png:aae/aae-0]]

** Action AutoEncoder

[[png:aae/aae-1]]

** Action AutoEncoder

[[png:aae/aae-2]]

** Action AutoEncoder

[[png:aae/aae-3]]

* Overview

#+begin_center
#+begin_xlarge
*Backgrounds*

*Latplan Architecture*

*State AutoEncoder (SAE)*
#+end_xlarge

break

#+begin_xlarge
*AMA_2 Overview*

*Action AutoEncoder (AAE)*

*/Action Discriminator (AD)/*
#+end_xlarge
#+end_center

* AAE does not address preconditions

 [[png:ama/overview1-1]]

* Action Discriminator is a Binary Classifer

*Binary classifier* telling which transitions are valid

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
[[png:aae/ad]]
#+end_span6
#+begin_span6
+ Trained by a *positive* and *negative* dataset

#+begin_center
+ We have *data for valid transitions (observed data).*
+ *Where are the negative (/invalid/) datasets?*
#+end_center
#+end_span6
#+end_row-fluid
#+end_container-fluid

** Invalid transitions

+ */Invalid transitions/*: *All transitions that are not valid.*
  + We lack the definition; *Cannot be generated.*

#+begin_container-fluid
#+begin_row-fluid
#+begin_span2

#+end_span2
#+begin_span6
[[png:aae/teleportation]]
#+end_span6
#+begin_span2

#+end_span2
#+end_row-fluid
#+end_container-fluid

+ *Teleportation violates the laws of physics*. (at least in a macro scale)
  + *Invalid transitions /never occur/, thus /never be observed/.*

#+begin_alignright
#+begin_larger
+ We lack *invalid transitions!*
#+end_larger
#+end_alignright

** PU-Learning framework (Elkan & Noto, KDD 08')

Learning *Positive & negative classifier* from *positive & /mixed/ datasets*.

+ Positive: Training Input. *Observed data are valid, guaranteed*
+ Mixed: *Successor candidates generated by the AAE*
  + Some are valid, some are invalid

#+begin_alignright
+ Now we can train the AD!
#+end_alignright

** Successor Function

AAE *enumerates* the canditates; AD/SD *filters* the invalid.

\begin{align*}
  Succ(s) &= \{t = apply(a,s) \; | \; a \in \{0\ldots 127\},\\
          & \qquad \land AD(s,t) \geq 0.5 \\
          & \qquad \land SD(t) \geq 0.5 \}
\end{align*}

# & \qquad \land SD(t) \geq 0.5 \\
# & \qquad \land Encode(Decode(s)) \equiv s \\
# & \qquad \land Apply(Action(t,s),s) \equiv t \}

* Planning using AMA_2

Using a symbolic forward-search planner

+ Effects/preconditions are embedded in NNs, but there are *discrete action labels*
+ Simple $A^*$ with *goal-count* heuristics
  + The number of different bits from the goal

* AMA_2 Experiments 1

Is it feasibile to do planning with AAE and AD?

+ *100 instances* for each domain
  + self-avoiding random walks from the goal state
  + (benchmark A) 7-step
  + (benchmark B) 14-step
+ 180 sec. time limit
+ Domain-specific plan validators.

# The failures are due to timeouts
# (the successor function requires many calls to the feedforward neural nets,
#  resulting in a very slow node generation).

# We next examine the accuracy of the AD and SD (\reftbl{tab:aae-results}).
# We measured the type-1/2 errors for the valid and invalid transitions (for AD) and states (SD).
# Low errors show that our networks successfully learned the action models.

** Results

Noise are applied to the planning inputs (init/goal images)

G: Gaussian noise, s/p: salt/pepper noise

+ *Easy instances: Majority of instances are solved*
+ *Harder instances: Still many instances are solved*


|   | /          |   < |     |   > |   < |    |   > |
|   | step       |   7 |   7 |   7 |  14 | 14 |  14 |
|   | noise      | std |   G | s/p | std |  G | s/p |
|---+------------+-----+-----+-----+-----+----+-----|
|   | MNIST      |  72 |  64 |  64 |   6 |  4 |   3 |
|   | Mandrill   | 100 | 100 | 100 |   9 | 14 |  14 |
|   | Spider     |  94 |  99 |  98 |  29 | 36 |  38 |
|   | LightsOut  | 100 |  99 | 100 |  59 | 60 |  51 |
|   | Twisted LO |  96 |  65 |  98 |  75 | 68 |  72 |
| / | Hanoi      |  37 |  44 |  39 |  15 | 18 |  17 |

* AMA_2 Experiments 2                                              :noexport:

How accurate are Action Discriminators and State Discriminators?


#+begin_container-fluid
#+begin_row-fluid
#+begin_span7
Measure the type-1 / type-2 error in %

#+begin_smaller
|          |    SD |    SD |   |    AD |    AD |   AD |   AD |
|          | type1 | type2 |   | type1 | type2 | 2/SD |  2/V |
|----------+-------+-------+---+-------+-------+------+------|
| MNIST    |  0.09 | <0.01 |   |  1.55 |  14.9 | 6.15 | 6.20 |
| Mandrill | <0.01 | <0.01 |   |  1.10 |  16.6 | 2.93 | 2.94 |
| Spider   | <0.01 | <0.01 |   |  1.22 |  17.7 | 4.97 | 4.91 |
| L. Out   | <0.01 |   N/A |   |  0.03 |  1.64 | 1.64 | 1.64 |
| Twisted  | <0.01 |   N/A |   |  0.02 |  1.82 | 1.82 | 1.82 |
| Hanoi    |  0.03 | <0.01 |   |  0.25 |  3.50 | 3.79 | 4.07 |
#+end_smaller

#+end_span7
#+begin_span5
#+begin_smaller
+ (SD type-1) :: Generate all valid states and count the states misclassified as invalid.
+ (SD type-2) :: Generate reconstructable states, remove the valid states (w/ validator),
                 sample 30k states, and count the states misclassified as valid.
                 N/A means all reconstructable states were valid.
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
+ (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
                 invalid according to SD or the validator.
#+end_smaller
#+end_span5
#+end_row-fluid
#+end_container-fluid

* AMA_2 Experiments 2

#+begin_container-fluid
#+begin_row-fluid
#+begin_span6
How accurate are Action Discriminators?

Measure the type-1 / type-2 error in %

|          | type1 | type2 |
|----------+-------+-------|
| MNIST    |  1.55 |  6.15 |
| Mandrill |  1.10 |  2.93 |
| Spider   |  1.22 |  4.97 |
| L. Out   |  0.03 |  1.64 |
| Twisted  |  0.02 |  1.82 |
| Hanoi    |  0.25 |  3.79 |

#+end_span6
#+begin_span6
#+begin_smaller
+ (AD type-1) :: Generate all valid transitions and count the number of misclassification.
+ (AD type-2) :: For 1000 randomly selected valid states, generate all successors,
                 prune by SD,
                 remove the valid transitions (w/ validator), then count the transitions misclassified as valid.
# + (2/SD, 2/V) :: Same as Type-2, but ignore the transitions whose successors are
#                  invalid according to SD or the validator.
#+end_smaller
#+begin_larger
+ *Reasonably accurate.*
#+end_larger
#+end_span6
#+end_row-fluid
#+end_container-fluid

# #+begin_larger
# #+begin_alignright
# + *Reasonably accurate.*
# #+end_alignright
# #+end_larger

* Conclusion

+ *Latplan Architecture* 

  „ÄÄ„ÄÄModels / solves the real-world problem as a *symbolic search*
  # + Input : Unlabelled pairs of images, initial image, goal image
  # + Output : Visualized plans to achieve the goal
+ *State AutoEncoder(SAE)* : */real-world states/ ‚Üî /propositional states/*
+ *Action AutoEncoder(AAE)* : */transitions/ ‚Üî /action labels/, /effects/*
+ *Action Discriminator(AD)* : */transition/ ‚Üí /bool/* with *PU-Learning*

#+begin_center
+ *Two of the major grounding problems were solved!*
  | Types of symbols      |                              |
  |-----------------------+------------------------------|
  | Propositional symbols | *Solved!*                    |
  | Action symbols        | *Solved!*                    |
  | Object symbols        | R-CNN (Computer Vision) etc? |
  | Predicate symbols     | ???                          |
#+end_center

** Future Work (Input format)

LatPlan is an *architecture* : Any system with SAE is an implementation

*Different SAE ‚Üí reason about different types of raw data*

+ Autoencoders for *text*, *audio* [Li 2015, Deng 2010]
+ Example:
  + Transition rule "*This is an apple, this is a pen ‚Üí oh, ApplePen!*"
    
+ */Latplan/* brings AI to a new level e.g.
  
  *1000 steps of natural language reasoning with /logic/, not by reflex*
  
  + Fundamentally impossible for short-sighted / greedy agents

#+begin_note
 "A hierarchical neural autoencoder for paragraphs and documents." (2015)

 "Binary coding of speech spectrograms using a deep auto-encoder." (2010)
#+end_note

** Future Work (Extended planning formalism)

+ Latplan assumes nothing about the environment machinery (grids, movable tiles...)
+ Latplan assumes *fully-observable*, *deterministic* domains
+ Next step: *Extending Latplan to MDP, POMDP*
  + Gumbel-Softmax layer ‚Üí just a Softmax layer? (probability)
    
** Other Future Works

Extracting rules from AAE/AD (discussed later)

Extracting much higher-order rule (predicates, HTN)

* This is the start of *neural-symbolic* AI.

[[png:latplanlogo]]

* Appendix

Using the Remaining Time (Discussion)

** Konidaris et. al (2014, 2015): 

Structured input (e.g. *light switch*, *x/y-distance*) ‚Üî unstructured image

Action Symbols (*move, interact*)

Just converting Semi-MDP /model/ to a PDDL /model/.

Could be used for extracting PDDL from AAE/AD

** NNs for solving combinatorial tasks:

TSP (Hopfield and Tank 1985)

Neurosolver for ToH (Bieszczad and Kuchar 2015)

*The input/output are symbolic.*

** Other Work Combining Symbolic Search and NNs

Embedded NNs *inside* a search to provide the *search control knowledge*

(i.e. node evaluation function)

Sliding-tile puzzle and Rubik‚Äôs Cube (Arfaee et al. 2011)

Classical planning (Satzger and Kramer 2013)

The game of Go (AlphaGo, Silver et al. 2016)

** Deep Reinforcement Learning (DRL) (Mnih et al. 2015, DQN)

DQN assumes *predetermined action symbols* (‚Üë‚Üì‚Üê‚Üí+ buttons).

DQN *relies on simulators*. ‚Üî Latplan *reverse-engineers a simulator*.

*DQN does not work* when it does not know *what action is even possible!*

** Other Interesting Systems

SCAN system (deepmind)

+ Maps *continuous* latent ‚Üî *human-provided* symbolic vector

Œ¥-ILP (Inductive Logic Programming)

+ ILP robust to noise
  + Extracting rules from AAE/AD to form a PDDL?

** Why not individual pixels? Why DL?

*Systems based on individual pixels lack generalization*

+ Noise / variations can make the data entirely different

  [[png:results/noise]]

+ must acquire the *generalized features*

+ = a nonlinear function that recognize the entanglements between multiple pixels

** Learning vs Planning
  
 Main differences: Purposes and the abstraction layer

 #+begin_container-fluid
 #+begin_row-fluid
 #+begin_span6
 *Machine Learning, Neural Networks* 
 
 for *Recognition, Reflex*
 + *Subsymbolic Input* (continuous)
   
   Images, Audio, unstructured text: 
 + *Soft Intelligence*:
   
   „ÄÄ */Reflex Agent/, /Immediate/ actions*
   #+begin_smaller
   *Pavlov's dog* : bell ‚Üí drool

   *Autonomous Driving* : Pedestrian ‚Üí Stop.

   *Machine Translation* : Sentence ‚Üí Sentence

   *Eval. Function for Go* : board ‚Üí win-rate
   #+end_smaller
   #+begin_larger
   ‚ò∫ Efficient 1-to-1 mapping
   
   ‚òπ Simple tasks
   #+end_larger
 #+end_span6
 #+begin_span6
 *Deliberation, Search*

 for *Planning, Game, Theorem Proving*
 + *Symbolic Input/Output*
   
   Logic, objects, induction rules
 + *Hard Intelligence by Logic:*

   „ÄÄ */Multi-step/ strategies*
   
   #+begin_smaller
   *Rescue Robot* : actions ‚Üí help the surviver

   *Theorem Proving* : theorems ‚Üí QED

   *Compiler* : x86 instructions
   
   *Game of Go* : stones ‚Üí Win
   #+end_smaller
   #+begin_larger
   ‚ò∫ Ordering constraint + complex tasks
   #+end_larger
 #+end_span6
 #+end_row-fluid
 #+end_container-fluid

+ AlphaGo = Subsymbolic (DLNN eval. function) + Symbolic (MCTS)

** Human-Competitive Systems

 AlphaGo = Subsymbolic (NN eval. func) + Symbolic (MCTS)
 + However, *domain-specific* -- specialized in Go, "Grids" / "Stones" are known
 + *Huge expert trace DB* --- Not applicable when data are scarse (e.g. *space exploration*)
 + */Is supervised learning necessary for human?/*
  
   *True intelligence should search / collect data by itself*

 DQN = Subsymbolic (DLNN) + Reinforcement Learning (DLNN)

Domain-independent Atari Game solver (Invader, Packman‚Ä¶), however:
 + RL Acting: Greedily follow the learned policy ‚Üí *no deliberation!*
 + You can survive most Atari games *by reflex*
  
 # ÂÆüÈöõ *Sokoban „Å™„Å©Ë´ñÁêÜÊÄùËÄÉ„Ç≤„Éº„É†„Åß„ÅØÊÄßËÉΩ„ÅåÊÇ™„ÅÑ* ‚Üî ÂÄâÂ∫´Áï™„ÇΩ„É´„Éê

** Latplan Advantages

#+begin_xlarge
*/Perception/* based on DLNN
#+end_xlarge

--- Robust systems augmented by the latest DL tech

#+begin_xlarge
*/Decision Making/* based on Classical Planning
#+end_xlarge

--- *Better Theoretical Guarantee than Reinforcement Learning*

#+begin_center
*Completeness* (Finds solution whenever possible), */Solution Optimality/*
#+end_center

--- *Decision Making Independent from Learning*

#+begin_center
*/Unsupervised/* (No data required), *Explainable* (Search by logic)
#+end_center

# ‰ªä„Åæ„Åß„ÅØNN„Å®„ÅÆÁõ∏ÊÄß„Åã„ÇâÂº∑ÂåñÂ≠¶Áøí„ÅåÂÑ™Âã¢„Å†„Å£„Åü„Åå *„ÇÇ„ÅÜ„Åù„ÅÆÂøÖË¶Å„ÅØ„Å™„ÅÑ*

*** When Latplan returns a /wrong/ solution?

 *Machine learning may contain errors* (convergence /only on/ $t\rightarrow \infty$, not on real time)

 + Images ‚Üí Fraud symbols/model/graph

 + *Optimal path on a fraud graph* or *graph disconnected*
  
   A* completeness, soundness, optimality (admissible heuristics) 

 + Fraud visualized plan (noisy) / no plan found

 #+begin_center
 #+begin_larger
 LatPlan may make */wrong observations/* but no */wrong decisions/*
 #+end_larger

 BTW, "correctness" is defined by error prone observations by humans anyways ...
 #+end_center

 #+begin_alignright
  (completeness, optimality) ‚Üí better reliablility than Reinforcement Learning
 #+end_alignright

*** Reinforcement Learning

Not only *perception* but *decision making also depends on training*

+ */Each training result does not have admissibility/*

+ */When the learned policy is wrong, the solution could be suboptimal/*

#+begin_quote
... AlphaGo was unprepared for Lee Sedol‚Äôs Move 78 because it
didn‚Äôt think that a human would ever play it.

#+begin_alignright
Cade Metz. "In Two Moves, that Redifined the Future." /Wired/, 2016
#+end_alignright
#+end_quote

#+begin_center
#+begin_larger
RL may make *wrong decisions*.
#+end_larger
#+end_center
** Future Work (SAE)

SAE can generate propositional symbols (state $s = \{q,r\ldots\}$)

+ 1st-order logic (predicate $p(a,b)$ )

+ We need *object recognition from images* (parameters $a,b$)

+ SAE with networks for object recognition (e.g. R-CNN) should achieve this

** Why symbols?

Symbols are strong abstraction mechanisms becasue

+ Meanings do not matter ::
     You do not have to understand it: Does a symbol $X$ mean an apple or a car?
 
     Logical reasoning can be performed by mechanical application of rules
     
     + Domain-independent planning : *mystery* vs *nomystery*

       Logistic domains where *symbol names are mangled* (truck ‚Üí shark)

+ Composable :: 
     A latent vector is a conjunction (and)
     
     Heuristic functions use modus ponens to derive guidance
